# 2024_in_review__stem_ai

(under construction)

2024 AI Year in Review

Project Management & AI ML: Old Books & the Genie in the Lamp, 
2025.01.19-2025.02.??

clearer title...

# macro-topics

(combined)
- structured-stateful-project vs. single-unstructured-stateless-reaction

(separate)
- structured vs. unstructured
- reactive vs. active
- stateless vs. stateful
- single-query/single-action vs. project
- internal vs. externalized data
- single train ("pretrain") vs. continuous (life long) learning

- what does STEM mean here

- Fantasy
- Search
- Economics of AI
- 2022-2026 impact vs. predicted/anticipated impact
- alignment, bad-behavior, short-term vs. long-term

- performative entertainment vs. STEM
- cargo-cults and gangs vs. STEM

- health & systems 
- prioritizing from the already not simple history of 'humans' and 'ai'
-- ELIZA and the Eliza-Effect
-- evolutionary systems


-- fuzzy logic
-- CA's
-- Turing's Biography

- A Sad Low Bar: Quality Criteria: not very good and measurably better than 'humans' (driving safety, medical search, eduction-support) 

- resources and energy costs...

important concepts:
- participation vs. gang-id
- project areas
- system collapse
- externalization
- generalization
- learning e.g. non-automatic learning

- confusion around unclear jargon for 'chain of'


basic concepts sections:
- what are vector-embedding/concept-matrix elements
- what is 'context-size' and how does it matter?
- what does 'pre-trained' mean? (how is pre-training not part of ongoing re-training learning)
- how does pre-training differ from empirical and goal-based outcome-based learning? (e.g. how are 'mistakes' learned from or not?)
- what is known-class-classification
- confusion around "chain of thought" and "reasoning"
-- one-shot 'chain of thought reasoning'
-- 'brainstorm mush' "reasoning"

- the prominent instant-automatic-complete-passive-general human-learning hypothesis is wrong
- 'generalization' is in limbo

- tech-stack landscape and timeline in 2024
-- matrix parallel compute

- Definitions and Types of (or approaches to) 'AI'
-- fuzzy logic...

- Definitions and ~levels of 'machine learning'

- types of learning:
-- outcome based and reinforcement learning vs. pure-regurgitation-learning
-- point-perspective-oriented//point-of-view-oriented learning
- alignment, non-automatic-skills, and project-state



The mainframe analogy, more than just me: Steve Gibson:
https://twit.tv/shows/security-now/episodes/1052 


... 

## Research in a post-bretton-woods world:

One of the trends that has accelerated (though it may reverse again) in 2025 is shifting and potential shifting of the status-quo post-wwII bretton-woods world where the USA, Europe and Japan etc lead university and public-fund driven research, supplemented by private sector and non-oecd research.

But a Mainland-'China'-Continent that is subsuming Russia and a USA that is reverting to a pre-WWII anti-Atlantic stance, and a USA that is becoming anti-STEM paranoid and pulling back from basic STEM infrastructure, aerospace and navigation, such as atomic clock maintenance(? NIST/NCAR), general economic data recording, science funding and University support in general, may at least indicate a potentially very different landscape for research, where the already somewhat fragmented world of matrix-parallel processing and operating systems may become otherwise fragmented between Chinese, European, and ever-diminishing anti-STEM USA (not to mention the volatility around Taiwan and Taiwanese chips).

[2025.12.24]

...

What are the technologies, what are the products?

...

unrealistic expectations and fantasies of 'convenience'


...

STEM, Progress, Feedback, and efficiency, vs. delusions, unmaintainable costs, and avoidance of project-area due diligence

A. 'diversions' on the side after work: recreation, art, culture; known to be full, sandboxed-fantasy

B. malfeasance an incompetence in responsibilities: populism, imbalance, and war; a violent divergence of the mind from reality resulting in physical a feedback loop between destruction-of-perception and destruction-of-infrastructure (as when a severely inebriated person becomes unable to manage (or be aware of) their inebriation)


...


# Fantasy and Chaos

threshold of technological difficulty, people getting embedded in silos of static, unmaintainable, soon to be abandoned, technology in isolation, and the propensity for fantasy and generally not-doing-homework is resulting in what will likely be several confusing years of waste, fraud, and abuse, after which there very well may be some useful technologies that emerge.

- You need to study
- You need to communicate and coordinate
- You need to do hands on project
- You need to articulate as part of critical thinking
- You need to move towards health-food study and projects and best practice; junkfood-code and bad-practice will be a crippling liability

People are not doing these things, and they are saying and proposing ideas that are simply not based in reality; and the default gang-culture way of handling this chaos is to bluster, defend your pride with force, blame and fire the intern, double down and double-dog-dare, be strong and wrong, blame "foreigners," and grab something familiar and turn it into a cargo cult.

This is not how computer science works. As Steve Gibson periodically asks: ~"What happens when you take the science out of computer science?" After years of thinking about this (I may be wrong) I am starting to suspect the answer is something like 'fantasy that is in conflict with reality, where reality prevails, and fantasy doubles down in fury.' 

But we have the choice to follow best practice and keep the science in computer science, and we should follow best practice.


This is not a 'doom and gloom' nihilistic stance, this is an optimistic 'conservative' stance. It is simply a fact that IT projects usually and notoriously are quagmires that go over schedule, over budget, and become unmaintainable liabilities that need to be replaced with the next over-promised quagmire. It is a fact that without project/product management of some kind, people will not be magically aligned and scope will churn forever into a vortex of ruin. But the fact, fact, that cowboys mismanage STEM and bully everyone in arms' reach does not mean that sensible people do not follow STEM and write effective software and manage projects and maintainably effectively and maturely. Many people, I think myself included, have been gaslit into a learned-helplessness apathy where we have given up and 'accept' a nihilistic cynicism that STEM and software are impossible and that everything is fraud and broken. But I do not think this is reality, I think that is an incidental tragedy of when and where some people have been alive. As I study and actually do projects (instead of trying overhyped vaporware products) I am seeing that STEM works, computer science exists, and real code can be written on schedule, on budget, and be maintainable. Manics will drown you in disinformation that everything is junkfood and assorted double-speak nihilisms and that you should give into your impulses and drink the coolaid because, they claim, "it's all coolaid anyway, drown your sorrows in distraction" but that is the echo-chamber of junkfood-noise. We have a choice. Some people are attracted to junkfood, though I am repelled. We can choose certain ruin and junkfood, or we can choose best practice. I choose best practice, and I am advocating for best practice and for more discussion of best practice. 

To try to clarify, if someone is doing a project purely for art, or recreation, or experimentation, or pure-research, I am not disparaging that. Culture and art are great. Not everything is a work-project with high-stakes consequences for the future. Not everything needs to be a long term maintainable utility. And some work-projects are short-term disposable projects. But we should try to be clear about which is which, and, just as we demand and expect of 'AI' we should try to be aware of our own 'confidence level' and alignment with reality. 

...


Landscape Status & Background: The ironies of 'religion' opposing 'environmentalism

Information Hygiene, information toxicity, disturbance-regimes, and best practice. 

While the Louis Pasteur and John's Snow's public hygiene revelations and the 'silent spring' environmental movement were literal, they are also a fitting paradigmatic model for what should become or may become concerns for system and information hygiene.

While Stephen Pinker is most likely correct in his many calculations of physical safety and increased education levels, and while concerns about modernity-villains are still phantoms in the foe-glass, there is (historical comparison aside) room for improvement when it comes to linguistic-cultural health, resilience, and productive-skill. 


...
"AI" as search, better search

"AI" as a re-envigoration for, a restored-enthusiasm for, STEM

AI and generalized STEM



# Product, Marketplace, Profitability, Summer-Winter Disturbances

search
big data
the sentient enterprise

the SQL & data silo apocalypse

Database Tech Stacks
Garbage Collection & Clocks
Database Cost

Data Reporting:
- making charts: A classic 'low-code no-code trap'

AI tools 'in' the marketplace: 
https://twit.tv/shows/security-now/episodes/1051 

...

# Is anyone still talking about "hallucination"?

- Topic: the strange topic of 'ai halucination'
one of the strangest things about what has been called or lumped-together as 'ai hallucination' is people's reaction to the term and the phenomena, such that it calls into question many of the complacent assumptions and "we don't talk about that" areas of human perception, communication, learning, interaction, decision making, psychology, etc. 

Fuzzy-guesstimation conceptual-space search.
vs.
Trying to use fuzzy-geustimation-search for a strict-definition structured-data query.

AI-hallucination vs. human-halucination. 

Has the world moved from being vaguely too-fixated on 'ai-halucination' problems to the other extreme of ignoring related topics, and vaguely believing that "they fixed that hallucination thing so let's go now fast wherever, but start now."


...


# Software Culture Landscape

- healthfood, fittness, study (not pejorative)
- art and recreation (not pejorative)
- sportsmanship and culture (not pejorative)
- traditional radicalizing extremist toxic sport (pejorative)
- disinformation, confusion, fads, and the popular madness of crowds; aspirational or paranoid (pejorative)
- pure research STEM (not pejorative)
- applied, integrated STEM (not pejorative)



...


AI impact...
- no job impact for four years... 

A. job-maker vs. job taker, or net-net even.
B. useless technology

Example of A/B testing:
- 

'used at work' vs. 'used by employer'
This kind of semantic parsing is maybe an indicator of the level of confusion and opaqueness that surrounds a nuanced topic and participants self-destrictively impatient with nuance. 



but people love to stoke doom-horror.


# Contents

- The Genie in the lamp problem
- Project-Space
- Deep-Fake vs. Deep-Fantasy
- Matrix-Compute Issues
- Abilities and Inabilities: Learning is difficult.
- 

- Recommended Reading
..
The Shoelace Problem
..
'private AI'


...

Enter the Matrix-Search

emphasizing matrix search: sub-symbolic search for machines is slow "system 1" intuitive guestimation

more on project management

more of other articles from the 2024

disinformation and labeling AI-made things

more on clearweb vs. deepweb AI

more on what tasks search-engine-ai can do

more on 'embeddings'

more on tech stack issue

more on lexicon issue

...

After the rocket-launch year of AI in 2023, the year of 2024 was less clear. I have been, often retrospectively (using materials from 2025), trying to piece together a portrait of where the world was throughout 2024 with an eye to what concepts we should wrap our minds around to better understand the world and how to get projects done well.

A few things did not happen until 2025, including the publishing or pre-release of books by notable authors such as Fransaoi Chollet (the creating or Keras and the author of Deep Learning in Python, 3rd edition in pre-release after his definitive refutation of the possibility of an AI boom published just before that boom took place in 2022-2023), Hobson Lane (author of NLP in action 2nd edition was scheduled for release in early 2023...and then had to be re-written, it finally arrived in 2025), and notable interviews and talks by figures such as Woodridge (who wrote (or published) a quite excellent book in 2020 about how the 2023 boom was impossible) and Hinton who famously retired and won a Nobel, and went on a speaking tour. Reporting by Journalists has marginally improved; The Economist has produced a few not-entirely-useless articles, which is good news.

But even late into 2025, the story of 2024 is still difficult to make sense of. We have, or had, little or no information about many areas, and at least apparently contradictory information about most else (from what companies were supplying to what consumers were demanding).


One of the most surprising...things...about the 2024 period is how vague it was and the quandary of how so many issues remained inconclusive or borderline and undiscussed for so long. 

There may be a corollary to the tautological question: "What happens when an unstoppable force meets an immovable object?" in the form of "What happens when something that cannot move quickly, such as the human ability to perceive and learn, is faced suddenly with a largely novel situation?" 

An irony, or more pragmatically, an application, of this question is the mirror-vortex of biological humanity being unable to rapidly learn about the AI that it is creating and using to try to rapidly learn about both itself and the AI that humanity is creating. There are perhaps some deep observations to be had in 2024 about several areas of linguistics and mind-studies that had been collectively put on the back burner since ~1971 and a still not-existing education-ology. As with the space frontier shelved around the same time, we are after 2024 desperately looking around for who has the domain and process knowledge about areas so long silenced that most of the once professionals have died of old age. As Jeffrey Hinton says in his bold-gesture retirement speaking tour of the question of how language works "We have no model." The vast majority of people somehow convinced themselves and each other that no one should even study the topic. Hinton and a few others risked it all to defy consensus: Hiton got a Nobel for not giving up (and being smart enough to make working tech). The rest of humanity failed that test.


One of the main areas of questions is about the abilities and limitations of generative general foundation models (with or without retrieval and function calling). For some tasks performance may be acceptable, such as outlining a resignation letter, but as of 2025 there has been 
> a significant slowdown in improvement
> a significant increase in cost (or admission of real cost)
> a significant lack of AI based tools, goods, and services, other than clones of chat-GPT

and even a basic lack of agreement about whether general models are doing more than memorization (or how much more). 

As was warned in the predictions of repeating AI-Summer AI-Winter boom bust cycles of overblown promises and then overly-pessimistic rejections, both the hopes and fears of 2023 (as of 2025) have diverged from the significant but more possibly more moderate advance in technology: a sometimes reliable search and spell-checker vs. an all powerful deity. 

As John Snow Labs has been doing with medical NLP for years, real life applications may still be significantly large, long, expensive, and limited projects that are beyond the budget and attention-span of most users. Hopefully advances and uses will continue, such as dating old scrolls from Pompei. 

The challenge of separating our imagination from reality is almost as difficult as learning that we should separate our imagination from reality. 

Read the old books.

...

The 2022 general-deep-learning model revolution is not almost four years old, and conspicuously people are treating it as if it is a form of entertainment about which they are only as interested as the clickbait is thrilling.

Journalists and technologists and people more broadly have now had about four years to read, examine, experiment, and there is no sign that the general level of literacy has moved at all. 


...



...


Read the old books

see: "why so many IT projects go so horribly wrong"
https://www.economist.com/podcasts/2025/05/15/why-it-projects-so-often-go-wrong

https://www.economist.com/china/2025/05/25/xi-jinpings-plan-to-overtake-america-in-ai

https://www.economist.com/finance-and-economics/2025/05/29/how-might-china-win-the-future-ask-googles-ai

https://www.economist.com/finance-and-economics/2025/05/26/why-ai-hasnt-taken-your-job


https://www.economist.com/britain/2025/06/05/britains-ai-care-revolution-isnt-flashy-but-it-is-the-future 

# List of Topics:

Search, Stateless tools, & Project State
-- state, statelessness
-- 
-- Project Space

- workflow does not generalize across domains: in this case: from anything else to AI-ML-DS 


- Ongoing confusion over System-1 and System-2, 'external STEM action' and 'reason' vs. overall signal processing.
- IoT  (economist article)

- meta-data and labeled data: what can and can't machines learn on their own?
-- learning a heuristic?
-- learning to solve and unsolved problem?
-- learning to navigate empirical results?

are foundation models another category: 
- memorization...of concepts?


- No Supervillain AI in 2024
-- changing stance on throttling being top priority

- areas:
-- language vector embedding models
-- language generative
-- multimodal generative
-- TTS
-- SST
-- PDF to structured

- econ:
-- large cloud is very expensive
-- in-house is very expensive

- people want stateful experts

- delinquency, best practice and STEM literacy
- No Municipal or Public Education Support
- Terminology:
	- "LLM" vs. "foundation model" vs. "deep learning"
	vs.
	- generative and vector subsymbolic concept-matrix models
- Overreach & Overlooked
	- GPU
	- M-Silicon
- Psychology & Linguistics Renaissance
- Clearweb Deepweb
- Fanticism
- clearweb-deepweb:
	- safe commercial/private-sector use
	- safe ~municiple/public-sector use
- reification-obsession
- bureaucracy and bad code
- Back to Pigmalion: Shaw & Chesteron

- statelessness
- Matrix Spaces
- Externalization
- ai-system-architecture vs. single-models
- local/cloud
- RAG vs. fine tuning
- graph databases
- model formats:

- interfacing structured and non-structured information

- file-path-space

- future proofing, sustainability, and 'technical debt'
- Rapid Change & Adjustment Periods:
-- Zombie Bert
-- 2-year delayed 2nd Edition of NLP-text

- Low Quality & Unreliability of Science/STEM Journalism
- Lack of Historical Knowledge
- ObRxSpc
- testing: STEM-Net Benchmarks & Synthetic Data
- Overly Vague Discussion of Synthetic Data
- disconnect between claims-hype and evidence
- sustainable technology and dead zones
- Khaneman Tversky
- timeline of science, timeline of history...
- Nearterm vs. Long-Term Architecture
- From 1800's Modernity-drama to 1940's Norbert Weiner & Gorge Orwell: Why project-managory, psychology, learning, and decision-making matter.

- Interfacing Structured and Unstructured Data, & Related processes

- The Economics of AI, The Economics of STEM


Stateless Technologies

Project State

Non-generative vector Space

Parallelism

Long Term Data Management

Deep-Fantasy over Deep-Fake

Sustainable Technology

Deep-Web-AI vs. Clear-Web-AI

Reinvigoration of Social & Cognitive Sciences

STEM

- Testing and Training:
Meta-data & learning: Topics where the raw material describes failure, and the goal is do better than that average-failure-state.

- Case Studies:
-- AI case study area: A. Replacing Humans, B. Tools for Humans, C. Nothing at all
Video games / 'tv games'

A. are mass-marget big game companies firing human developers and simply using 'ai bots' to make games instead? 
B. Are independent (human) game-makers using AI-tools (in the way they would use interns or volunteers) to make more game-materials more quickly (either more games, bigger games, or faster development time)? 



(more history books, especially William L. Shirer books)
- Leviathan, Hobbes
- This is Berlin, William L. Shirer
- Berlin Diary, William L. Shirer
- End of a Berlin Diary, William L. Shirer
- Rise and Fall, William L. Shirer
- Fall of the Third Republic, William L. Shirer
- The China Mission
- First Principles
- The Rise and Fall of American Growth
- Slouching Towards Utopia, B. DeLong
- Black Earth, T. Snyder
- The Bloodlands, T. Snyder
- Reappraisals, T. Judt
- Thinking the 20th Century, Judt & Snyder
- Red Famine, Anne Applebaum
- In these Times, Jenny Uglow
- The Making of Modern Corporate Finance, D.H. Chew
- The Measure of Reality, A.W.Crosby
- Embeddings in Natural Language Processing, Pilhvar Ed. et al (springer)
- Franklin & Winston, J. Meacham
- Mr. Churchill in the Whitehouse, R. Shmuhl
And always read first folio Shakespeare, e.g. Norton First Folio
- fads:
-- mamba
-- mixture of experts

...


# Economics of AI

A. GPU Economics
B. Open source vs. commercial
C. Short term vs. long term: maintainability
- Longevity of GPU
D. Over-investment, Mal-investment
E. Hybrid-structured/unstructured infrastructure costs



The Economics of AI are taking shape, and are still taking shape such that from 2023 into 2025 what the economics of AI will be is still very much a moving target. 


One overall pattern is that across the board people know so little about AI that it is difficult to make any products that people want, will use, and know how to use.

It is very very difficult to work any AI (old or new) into any product. Customers don't know what they want. The tech-stack of spamming out cloud services based on backend frontend and endpoint is not suited to clear-web or deep-web AI, everything is extremely difficult and expensive in time and resources, and everyone seems to be expecting everyone else to pull a rabbit out a that which results in everyone standing around doing nothing. 


(part of economics of AI)
# Product, Not Market

https://www.economist.com/international/2025/11/06/a-new-industry-of-ai-companions-is-emerging 


As in this article, which illustrates how little has changed since Thomas Hobbes wrote his analysis of scripture (not typo), 
https://www.economist.com/middle-east-and-africa/2025/11/07/hemedti-warlord-power-broker-and-the-new-sultan-of-darfur, while Francis Fukuyama may have compellingly argued that a STEM based enlightenment social contract is preferable to the default status quo: gen-AI does not seem to have altered the timeless default horror.
..
word of the year for 2025: slop


Topic: What is there to adopt and what is the pace of adoption? 

from https://www.economist.com/the-world-in-brief 2025 12 25
```
Tidings of joyless-corporate adoption
Three years into the generative-AI wave, demand for the technology looks tepid. A recent survey found that only 11% of Americans were using it at work, down from recent peaks. Tech firms plan to spend some $5trn on AI infrastructure over the next five years, an amount which will require $650bn a year in revenue to justify—around 13 times today’s sum. Without rapid and wholesale adoption of AI, those returns will remain out of reach.

What’s behind the slow uptake? Some point to economic uncertainty—businesses may be feeling more frugal. Others point to the AI models themselves, which may just not be good enough yet. Others still blame managers, who may be bad at encouraging their use.

History teaches that technology adoption spreads in fits and starts. Still, the current plateau suggests that AI’s economic pay-off will arrive more slowly, more unevenly and at greater cost than the investment boom would imply.

Word of the year: slop, once a term for watery gruel, has become shorthand for the AI-generated rubbish clogging the internet—from fake videos to autogenerated articles. Read the full story.
```
...

# Fantasy

## Risk, Fantasy, and the known history of difficulties and issues with IT and computer science projects

In pushing the frontiers and aiming for lofty hazy goals (such as 'improving better-ness' or anything connected to profoundly undefined terms such as 'intelligence' or extremely not-agreed-upon terms such as 'alignment', especially when they make zero effort to look at existing domain knowledge and research about industrial and biological systems or learning and education) some developers are knowingly and legitimately pushing the limits of what is known and done because, that's how you learn and do, and they are keen to see what they have learned and done. Other researchers and developers, based on the largely trainwreck history of computer science in terms of failing to learn and instead repeating the same mistakes of practice and policy indefinately, are engaging in something like a sport of fantasy in economic bubbles and the madness of crowds. In markets and business, 'irrational exuberance,' especially when compounded by not having accurate 'rubber-meets-the-road' feedback about reality, allows a combination of imagination and belief to attune people to a hypothetically-possible opportunity. Sometimes this turns out to be (directly or indirectly (after adjustments)) a viable path, sometimes this turns out to have been a perception-error.
As the madness of crowds goes in both directions, fear and nihilism can also be an overwhelming fantasy-belief-fiction-nightmare. From 1969-2012 the vast majority of the world was in a 'winter fantasy' where they were locked into an unshakable fictional-fantasy-belief that sub-symbolic-STEM and neural networks were not pragmatically viable. This was an 'AI Winter' fantasy that was wrong, of the ~negative sort. 
One thing that books written before 2023 often are adamant about is the risk of both AI-Winter fear-delusion-fantasy and AI-Summer hope-delusion-fantasy. Decades of research possibilities were lost and wasted because people chose the sport of fear. And likewise many research resources have been wasted on hope-delusion-fantasy project such as hand-made knowledge-webs.
Both fear-delusion-fantasy cults and hope-delusion-fantasy cults are perennially attractive to people (perhaps to AI as well?), and it should be a policy and a priority to be vigilant against the harm they cause.

One low hanging fruit approach is to talk with someone about the project-areas 
- https://github.com/lineality/project_areas_for_project_and_product_management
- https://github.com/lineality/Online_Voting_Using_One_Time_Pads
- https://github.com/lineality/object_relationship_spaces_ai_ml 
of a given project. If they are most interested in cult-beliefs and adrenaline rush thrills, that may rise to the surface. If they are cognizant of and interested in details and risks pragmatically, that too may rise to the surface. Note: Life is mixy; people who make important discoveries are not always entirely sane and without luck.

...

?The Economics of AI?
# Uses & Investments

While foundation-model deep learning technology is getting somewhat better in a few ways, it is still not clear what clear affordable business uses there are, and it is not clear what effective uses there are. 


I am a reader of The Economist (for better or worse). Up to 2025 there were very few and often very low quality articles relating to AI/ML DS technology (which is admittedly not intended to be the main research focus of the Economist). In 2025 the number of articles in general has gone up and the quality of the articles has sometimes gone up considerably (there is still the occasional articles that is only posturing, arm waving, and emotive sound bytes). 

The information coming out is largely peripheral, and there is still no clear sign (as is my understanding) that there is any prominent useful application of generative AI at all. They reported an MIT finding the 95% of investment has produced nothing, which raises the unanswered question of what that supposedly productive 5% is doing. 

This should be an epistemological revolution. This is an investment rush. There is no clear substance outside of the eagerness to invest itself (a classic bubble, in a sense).

The thing is, this does not need to be a bubble, there does not need to be a VisiCalc 'killer-app' that makes profitable AI take-off. There can be myriad small uses and applications that help small businesses, municipalities, parents, students, in incremental ways.

There are illusion bubbles and there are over-reach bubbles. Around 2001 the 'dot com bubble' burst, but the internet and the 'mag 7' still had their revolution. The internet was not a tulip, it really did change how daily life worked.
Machine learning need not be a tulip. AI arguably is the intersection of all STEM areas, and as Stephen Pinker has convincingly (if not always politely) made the case in several books that STEM has had a significant positive impact on human society.

The uses mentioned are indeed advances of a sort and interesting such as reading 

- previously inaccessible dead sea scrolls, 
- 'SCOTUS-BOT' analyzing US supreme court rulings, 
- several articles and letters about AI-generated beer brewing recipes
- 

- People are using generative-AI for web search, indeed Google successfully argued this in court to convince judges not to split up Google into several smaller companies. 


What are people using AI for?
What are people using foundation-model generative-AI for?
What are people doing to profitably make money with generative AI?

The answer is as diverse as STEM itself, and has no dramatic single punch line. And like STEM, most of it is still a mystery to be uncovered in a future we may or may not be smart enough to live to make.


- high cost
- no plan for profitability
- no clear architecture build


The story of what is going on with AI through the lens of 2024 is a landscape of small puzzle pieces much more than one unified quick success fantasy, and the prominence of people's desire for a single entertaining fantasy is one of those puzzles.



...

# The Economics of AI, The Economics of STEM:

The economics of software on the whole are not clear: sold on disk vs. software sold online by licence (spinrite) vs. software cloud subscription vs. open source vs. free-tier, 

- disincentives to 'finish' software
- 


how is there no open source or NIST or university driven program to create and curate practical task coding and project state fine-tuning datasets?
- 

Nazi Chess and Chicago School Economics: At what point does short-term thinking become suicidal?

tragedy of the commons
profitability
macroeconomics and debt
economics and social institutions


General Overreach and Misreach:
- databases
- spreadsheets
- mirages
- political abstractions
- 


Designing maintainable products is not easy:
- architecture
- law
- textiles
- 

2023 general-foundation-models and maybe an end of the 1970-2020 ish set of paradigms are happening in an overlapping way. 
more abstracted these two do not have to be associated, but for use they most likely will influence each other due to real proximity.

a common theme is perception of systems:
yes, biological homo sapiens humans (~people) are uniquely able to do some language and other tasks,
but we are still not good at perceiving, communicating about, or mapping out what we are and are not good at.


A Confusion of Paradigms: 
Chicago School Economics vs. ESG vs. Ecology

profitability:
charity business or confusion:
investment in productivity boosting is likely a good idea
but that does not appear to be a high priority generally.
Hear any talk about 'life lone learning' and 'adult education' at CES-2025?
or workforce fitness management?






Shrines vs. Factories


[question_word] [pejorative_term] [preposition] Munich Agreement in 1938?

What went wrong in / at / around the Munich Agreement in 1938?
no one

decisions and coordinated decisions need to be on the menu
project management and alignment need to be on the menu.
foundations and sustainability need to be on the menu.
managing popular destructive behaviors needs to be on the menu.


Context windows are a big topic, relating to stateful or stateless tools
- tiny context window create artifacts of extra-statelessness
- big context windows can create pseudo-state big ~caches of background that can be useful but are an illusion of state-in-time

both of those are likely to create mirage impressions in users


GPU-clustering is not adding up yet:
while cpu clusters on devices are now normal (15 years ago less so)
adding GPUs to a system is a basic requirement which is strangely lagging behind: the foundation model movement started in late 2022, yet in early 2025 there is no discussion and no progress? That seems strange. An analogy might be claiming that everyone will own a cars soon but not starting to build roads. In the case of early fords in the USA, according to 'rise and fall of american growth' part of what made the model T ford popular was that it didn't need roads, e.g. for rural postal delivery.

And a similar dynamic may be happening with CPU-only foundation models (but strangely not embedding models for the most part, which is another fundamental disconnect ) 




...



must-read computer-science & stem history books:
https://docs.google.com/document/d/11DFQtsNjrqHENS0D7UpuZhOhcqCKK39JfmEBc8O8NHI/edit?tab=t.0 
or
https://github.com/lineality/cs-computer-science-curriculum/blob/master/Data%20Science%20AI-ML%20Book%20List_%20Recommended%20Background%20Reading%20(shared)2025-04-13.pdf


..

AI Fantasy appears to be what is on the menu. People want AI to entertain them.
..


past articles:

recommended reading:

...

Sections by Topic

...

Skip Ellis & Assistants for Teams
- helping with organization
- helping with groups interactions

...

What was new in 2023? 


...

# What happened to 'generalization'?
For better or worse people have stopped talking about it.

...
AI & Ethics (STEM & Ethics)
- no 2024 emergencies
- no 2024 guidance either


Theme: 
- pre-2023 neglect of ethics
- pre-2023 neglect of psychology

...

So far no significant intersection between either general social-media obsessiveness or violent-organization-recruitment.

...

No 'review of the literature' yet,
up to 2023 books said 2023-25 AI was not possible,
then people just stopped trying to describe AI...
???
that seems strange to me.

I've been writing articles and sharing code but so far I have not seen any interest in discussion.


...


Topics:

# Search, Structuring, & State: The unsolved problem of search:

Clear Description Matter

From 'search' to 'chat' and back again.

Does "AI" = 'Better Search"?

'The unsolved problem of search':
Both search and statistics are topics that seem to keep expanding in very STEM ways over time, though socially we want to pretend that they fit in nice little boxes and have long been fully 'explained' (or explained away). 
The story of search (as with the story of statistics, and quite possibly the two are related) is not done and dusty in 2025, but quite likely rumbling and gurgling like a volcano. 

In retrospect the 2023 'ai revolution' may be seen and described as 'the search revolution' as a 'search done better and possible locally' breakthrough, which largely came out of google (search giant) technology (transformers) and operates largely like the google search page paradigm, except the search button to the right of the query-boxy now says 'chat.' 

Calling search 'chat' may be an example introducing a needless and meaningless layer of 'bad naming' that causes personal and social confusion (as people reify the fog into mirages and phantasm), like rebranding gone wrong where people are confused about what product they are using.


TODO
Let's review embedding models briefly before looking again at generative models as a form of search.

note:
manifold hypothesis

Why is quantizing so effective?
In historical speculation, which is highly speculative, it is conceivable that if 'chatGPT' were called 'searchGPT' and the technology called 'search' not 'AI', and 'gen-ai' was 'generative-search' that the same technology and adoption and investment and innovation and enthusiasm may be processed without all of the misplaced 'super ai taking over the world.' It is easier to be afraid and panicked that a mysterious hidden ~'unnatural' intelligence is going to 'take over' whereas people historically are not panicked about 'search' taking over the world. "Search will take over" sounds a lot more mundane.

For example retrieval-augmented-search (or retrieval augmented generative search) is not a bad description for 'retrieval augmented generation' which confusingly ignores the entire non-generative vector-search part of the workflow.  

If we take a step back from the fiction that people are 'chatting' with a stateful full-intelligence, rather that doing stateless 'searches' with very wonderful search, smart search, vector search, generative search, technologies, things make much more sense, are much more clear, and avoid all the undefined nonsense around undefinable terms and the fantasies they invoke in people. 

A few issues are key here in the search context:
1. 'generalization'
2. 'explainability'
3. externalization
4. state and project-state
5. unstructured-to-structured conversions
6. tech stack
7. matrix-processing

(notice: undefined 'intelligent-ness' is not on that list)

If it turns out that super-dooper-search causes problems that would have warranted a broader discussion (if without the unhelpful panic), then what I suggest here may seem like a 'cover-up' attempt or naive. 

But so far, and for 2024, 'search' seems to be a much more accurately scoped context and description. While there may be aspects on both sides of this evaluation, it is looking like there is a lot of 'projected' imagination being woven into people's perceptions of what is happening, perhaps not unlike very young children who intermingle their psychology with their descriptions of stuffed animals and pots and pans around the house. 

...

For 'model' 'explainability'
From Example of MYCIN:
Is 'explainability' really about project-alignment, project scope definition, and project management, project context? Such that the less project-context-definition there is, the less 'explain' is meaningful and definable. For example, can you define 'explain' before you define the project that needs to explain something? 



- Where are we?
- Foundations Matter

- Alternatives to Transformer Architecture?



# Pretraining, Retraining, Continuous Training, & Feedback Training


History and Process:
- What is the nature of collapse?
- What is the nature of 'viral' impulsive popularity? 
- 


Information epidemiology and contagion
- It is too late in history for people to be naively trying to cultivate virally spreading popularity bubbles as though that were constructive. 



In 2024:
- Sweet-Spot: Small Enough & Fast Enough
- "Disambiguation" from Anthropic CEO
- Economics of AI (taking shape)
- Deepweb-AI & Apple's Strategy
- The 1930's Again
- Upgrade to MMLU


Speculations & Observations:
- Process: Project & Alignment Skills
- Learning & Repairing Bad-Behavior
- Training Data
- not over but not easy
- Threshold of Ignorance
There appears to still be massive misunderstanding about what old and new AI can do and how it works. 
- Tech Stack Liabilities
...



# Missing in 2024:
- Investment in Science Journalism: "Let them eat clickbait."
- 'What happened?'
- Tests, "Evals," Benchmarks etc.
- "Deep Fantasy" vs. Deep "Fake"
- AI Corpus Callosum
- Architecture
- Externalization
- no edu applications
- Where is ONX? (What is ONX??)
- gpu clusters: hardware, software, discussion
- "pretrain" confusion and continual-training debate



...

While it may sound non sequitur, the big word in AI after 2024 is... "Project Management." A well known phenomena for AI developers and users is the 'Genie in the Lamp' problem: Everyone knows the story of the magic lamp that will grant anyone three wishes, where the moral of the tale is that everyone spends all their time asking for the wrong things. AI ML is a potentially wonderful tool, but, just like with the genie in the lamp, the task of figuring out very precisely how you should use it and what you should ask for, and how, is perpetually underestimated (despite everyone growing up familiar with the moral of the genie lamp story) and bedevils many a bedraggled project. 

For me this theme unfolds, or folds back on itself, so that the question is not just how to use unaffordable armies of uncoordinated people to fail to plan AI projects, but how we might be able to use AI and STEM to better manage projects (e.g. project management benchmarks for AI)...again, a topic that is stubbornly off people's radar: "managing projects" just does not stick in our heads as a category of project that we should do and get better at doing. (How many  people told you that their New Year's resolution this year was to prepare to be able to write more and more measurably effective resolutions each year?)

And (for me) this has additional twists, as this kind of AI-Project-helper project was how I got my start with AI and NLP back at CU Boulder in the School of Engineering (computer science department, with Prof. Skip Ellis in Project Neem). 

For years in data science the lamp-genie problems have been stand topics in courses and workflow. And now that data science is becoming a set of tools and concerns more broadly used and faced by the public, it makes sense that same process concerns are now more broadly public as well. 


...

Hobbes, Derrida, and a Grammar of Worlds and Stories


# Concepts and Analogies
Fractional reserve banking & The Derrida Problem

In addition to a mysterious ending conversation about Mandelbrot's models of markets, there was a flurry of discussion about the question of fractional reserves following the 'Lehman-shock' financial oopsies of 2006-2009, a preventable series of events that may have significantly contributed to the subsequent re-emergence of international-scale populist extremism that had been sufficiently dormant since the end of the second too big war that people had so believed extremist radicalization in the OECD had gone forever that there was (and still is, somehow) a notable retisense in acknowledging that this problem once as solved as polio is now back: perhaps we should remember the pleas of Cornelius Fudge: "He can't be back. He just can't be."

The problem of not being connected to reality will probably never go away, nut that does not mean that we can't learn to recognize it. 
.....

What has changed and what is possible now that wasn't before?

What is a reasonable scope?

There is a mix of people legitimately not knowing because they have to test
and people lost in fog-of-project confusion with people making ungrounded 
claims and pushes that turn out to be mirages and vaporware.

my general stance is that NLP is especially cumulative in STEM, and so
the old tools have not changed, but some new things have been added to the
big interlocking 'tech stack', like placing a new lego in a lego city.

So if the old items are still there unchanged, and something new is added,
what has been added? And what hasn't been added?


1. general foundation model of type meaning-concept-vector
2. language/media generating deep learning vector models
3. vector-locating or vector-output meaning vector models "embedding"


Q: foundation models for iOT data?
Uses of vector models?

1. foundation models:
something has changed in what kinds of data a deep learning model is able to model,
(this has been the overall focus of my 2021-current "object relationship spaces"
looking at project system state within architectures


but broadly speaking, there is not a great deal of prominent discussion of any kind about this (and you can't evaluation the qualities or lack of quality of a lack of discussion). I think it would be helpful if this topic were more discussed. Such as discussion would not need to be, and should not be, confined to a small number of technical experts and about arcane details. The book 'Possible Minds' 25 essays about Norbert Weiner's 1949? book the human use of human beings first edition may be a good example of how a diverse discussion of people from different backgrounds and disciplines make for a more well rounded and accessible discussion. 

2. this is where most focus has been, with 2nd place focus being on image generation.
But for what people use this for, the overall answer seems to be:
A. nothing in particular and
B. no one really knows, and the main entities involved are companies who
perhaps understandably are not putting effort in to even general anonymized reporting on this,
(note: I am not anti-private sector, anti-company, anti-big tech, and 
arbitrarily anti-copyright). Any maybe there is no pattern to report,
but we are in the dark in early 2025. 

3. vector models: no one seems to be interested in this question,
which seems unthinkably odd to me.

...

# What are we supposed to be planning for? Nearterm vs. Long-Term Architecture

A painful example of the difference between nearterm and long term architecture equilibria is the DOS epoch (now effectively both extinct and blockaded) vs. the posix-paradigm, over the x86-32(bit architecture) to x86-64(bit architecture). 

If you could go back in time to 1980 and give whatever advice you wanted to a software company, what would you say?
DOS was, by hook crook and bludgeon, the only dominant platform from mid 1970's to 2000. After 2000 DOS is mothballed and that lineage become un-use-able in several ways (DOS itself was abandoned (and also still strictly copyrighted) while 64-bit NT-based software is dysfunctional and incompatible with everything. Debian-linux is perhaps the most long-term-stable and widely-used OS for software development and CUDA compatibility.

The long-term-game is some kind of gpu-integrated linux/posix chimera that even in 2025 does not exist yet. According to Wikipedia, "Apple announced its plan to switch Mac computers from Intel processors to Apple silicon at WWDC 2020 on June 22, 2020." [https://en.wikipedia.org/wiki/Apple_silicon] Once again Apple Computers is ahead of the curve, possibly too far, with people outside of Apple being simply confused by the idea of cpu-gpu integrated memory 'way back in' 2020, in 2025 the idea of actually managing GPU computation-space should seem like an epiphany, but still no one outside of Apple seems to have put 1 + 1 together: and unless enough of the market gets behind an idea, it may just sit in a drawer or worse for decades or centuries (as the history of science is full of examples, from bayesian statistics, to copunicus's solar system, to mendelian genetics, to boolean digital logic, to corning's gorilla-glass, etc etc). 

So if you could go back in time and give 'key insights' to the computer industry positioning itself for the 2025 AI-Economy... the best they could do is simply invest in unsustainable technologies doomed to die in order to bide their time for future standards that still might never come at all.

The best we can do as of 2025 is to highlight the issues and factors, but since fads and legislation and funding trends and hyperbole change in a matter of weeks 'long term' prediction over decades is either too difficult or very discontinuous. 

In reality 35 years is a standard R&D timeline for starting work on a technology and then that technology taking root in the market, but the timespan, however stable, seems to be longer than both the median-human's brain capacity to imagine and understand, and even that of elite institutions and companies to effectively strategically invest in conservatively.


...

# Tech Stack Uncertainty:

- ibm-clone vs. macOS
- r, python, mojo
- go, kotlin,
- pytorch or tensorflow
- CUDA indispensible, or CUDA single point of failure?




ONNX Models: On the edge of abandonwear

? https://en.wikipedia.org/wiki/Open_Neural_Network_Exchange 




.....


# "Status Report?"
As a microcosm of 2024: The Manning Press book "Natural Language Processing in Action" (from March 2019, by Hobson Lane, Cole Howard, and Hannes Max Hapke) is one of my favorite books of all time (and I recommend everyone read it).. https://www.manning.com/books/natural-language-processing-in-action 

In a classic cosmic giggle of fate, the second edition (of NLP in Action) was about to be released in 2023 when the OpenAI foundation model breakthrough happened... and then the release date of the 2nd edition of NLP in Action was very fortunately pushed back (most likely because they wanted to reassess how much had changed, rather than just go ahead and publish no longer current book; e.g. material from 2024 was included in the book originally scheduled to be published in 2023, so not feasible to have been a delay in going to print with the 2023 version). Through 2023 and then 2024 the release date kept being pushed back. It is now 2025 and maybe it will be released in the first half of 2025. 

I think a conservative reading of this story is that the AI world changed a lot in 2023, and changed more than people thought it had changed. It took all of 2024 just to be able to make a status report on 2023. You can imagine the conversations between the author and the publisher between 2023 and 2025. 

"Should we push back the release date and make some changes?"
"Yes, I think so."
"How long will that take? How many changes need to be made?"
"We don't know yet."
"Just a ballpark estimate: a few weeks, a month, even six months?"
"We don't know yet."
"Do you just need to add a new 'afterword section' or...What has changed? Is the old material now wrong? Is there new material to just add?"
"We don't know yet. A lot may have changed."


There are similar signs that Academia is also undergoing longer-time adjustments to what is happening, here are two:

1. A general and suspicious silence: Academics who boldly published just before events in 2023 have since been extremely quiet. 

2. The BERT-Echo Mystery: 
There is a strange timelag that is almost understandable in terms of publications on state of the art generative and embedding models. Like a deer in headlights, the official model evolution timeline has been strangely frozen through 2024. What is the best in class AI model? Everyone knows the answer to that! It's the 2018 BERT model! ...really? 

For reasons that somewhat make sense, a lot changed very quickly and it takes years for larger research programs to finish and publish. So it makes some solid sense that there is a backlog of BERT research that rightly should finish and publish and that will take years to fully unroll. 

But the absence of updates is strangely very very quiet. In late 2024 I tried asking a few reasonably up-to-date cloud AI models by major companies about this: What are the most recent embedding models and technologies? It's the 2018 BERT model! ...really? Every big cloud AI insisted that yes, BERT was state of the art and that top tech firms such as Google and Meta were using it in core products. I could not confirm any of that with fact-digging and just on the face of it, it sounds very wrong. 

There are still lots of research papers like this, from Jan 28 2025:
https://arxiv.org/abs/2501.17330 

# Research has no 'arabidopsis/drasophala' of open source models for research: Zombie BERT

It is possible that there needs to be a completely open model, training, training-data, hardware configuration, testing etc. ecosystem, in order for academic research to operate or operate well: doing and comparing studies that can be reproduced (note: reproducible research is key). The gulf between the entirely closed top performing models and the incomparably different and worse academic models is a research issue.. 

Again, I think a conservative reading of this story is that the AI world changed a lot, very quickly, and changed more than people thought it had changed, in 2023. Another issue may be that a lot of research is now being done by companies and for various reasons not made open-source. While some researcher-companies such as Meta and Mistral are ~more open about their research, Antropic and OpenAI arguably have both the strongest models and are completely opaque closed systems. Google publishes some data but mostly is only open about their smallest and most useless products. (

(Update note: As time moves on, statements made at time of writing need to be updated. In August 2025 OpenAI ("Open" AI) released a set of open model for (according to wired https://www.wired.com/story/openai-just-released-its-first-open-weight-models-since-gpt-2/ ) the first time in 'over five years'. 

Note: Why is gpt3 still not open in 2025? Many models by many companies remain closed for inspect, use, study, etc. long after they are moth-balled as being not worth using. Is there a reason or cause for this beyond unthinking uncoordinated random unjustified behavior (such as one can observe in a common non-agile software project that devolved into complete nonsense until it collapses and is abandoned for the next not-planned, not-using-process, disaster)? 

The original modification or imperitive, if I recall, for models to be 'closed' (note: 'open model' and 'technically open source software' are not the same sets of concepts) was security, the 'AI Supervillain Hypothesis' that I attempt to discuss in this paper.' Even if people are still arguing (though in 2025 people appear to have become bored and moved on from the panic-hype-farming around chat-bots being highly useful for criminal doing tasks but somehow useless for the same tasks when applied to everyday life and work projects) what is the justification for keeping 'old and useless' models (e.g. models that are not even available to use at all as a paid service because the performance is so bad) closed? Is the old security claim still being used, that people would use gpt3 to take over the world if it were made open?

A similar question arises for all the effort that companies such as google, anthropic, openAI, and also Nvidia, salesforce, etc. who have closed embedding models: Is the strategy of putting lots of time and energy into preventing people from using a model that is only 'current' for 1-6 months before being moth-balled a working strategy? If companies cannot even test or experiment with a product, how does that help adoption? If Gemini-1 (many many many iterations back over just a few months) had been an open-model, what would Google have lost? 

A significant factor here may be the GPU-TPU matrix compute and cloud compute 'challenges' ('nightmares' might be a more accurate term). Even if a set of companies wanted to 'use' an "open" model, how many of those companies would not want to pay for help with the huge sprawling tasks of GPU/TPU-running those models and training fine tuning, etc.? 

This is not intended to be a shallow paranoid suggestion that a profitable model making project is somehow nefarious or indecent. The computer industry is, compared with other sectors that already had mature trade guilds in London when uncultured Roman emissaries wandered in, very young and it is not entirely clear how best to maintain technological skill and development. From Univac to the Apollo 'Moon Shot' (deliberately sabotaging a long term program for a short term goal...that sounds familiar) to the DOS, consoles, and Java ecosystem extinctions, and both the internet and AI R&D being based on linux (C-linux), and the C++ OOP epic disaster, there are few data points to learn from. Selling software (that needs constant security patching) on disks in a physical store did not work out. Even the first generation non-subscription Android apps is mostly extinct. As Steve Gibson has expressed, the business model for software is not clear when what is paid for is fixes to software but what has value is software that isn't broken. Why would a company invest decades on 'complete' software that will not generate future growing profits? It may be another category of tragedy of the commons, where people need mature software just as they need all STEM related tools, but perhaps like raising the next generation of humanity ('children') there is no profit-growth investment alignment for that... so people don't invest in it and everyone is very significantly harmed by that lack of investment. "Lear, Lear, Lear." Is there some way to align incentives so that needed investments are made? Ecology loving 'Open and Sustainable' people rage against any discussion of profitability (for some reason), and finance and economics loving 'viable business plan' people rage against 'donating to good causes.' Unless we can make this engine click and turn over, our past record of self-defeat is unlikely to change without cause. If ants and slim molds can solve population logistics challenges, then surely hominids can rise to the same lowly challenge.

In some ways people underestimated the change but in other ways people over-estimated the change. For example, and this might be an extra factor in both the silence out of academia and pressure on companies to not-open-source, there was a significant cloud of fear around using or even researching AI through 2024 (though there may be some evidence that this is lifting in 2025 with an emphasis changing from suppressing AI to making AI that actually works to do something). If I recall correctly, when Meta opened-sourced their models it seemed like a daring, possibly illegal, thing to do, and most people applauded Atrophic and OpenAI keeping everything under seal and key "for the greater good." But in 2025, with a bit of Monday-morning-quarterback thinking, Meta is seen as being obviously right to open source their models, and Anthropic and OpenAI are considered to be focused on legislative monopoly goals for pure anti-competitive business profit. That's a lot of "change" in one year over something that didn't actually change: meta was and is open, Anthropic was and is closed. 

Public opinion on OpenAI has not weathered well. There seemed to be a good-will honeymoon period early on for a while after the popular release of chat-GPT. The not-open-source models at first seemed to be, by and large, supported by a public fear that in retrospect should seem absurd: There is no evidence that GPT-2 to GPT-3.5 (or even GPT4) are capable of either 'rising up' and destroying the world on their own or allowing bad actors to do the same; for example, more powerful open source models have since become available and there has been no tsunami of robot-overlord human genocide (or... even a profitable company using the technology). But, perhaps circumstantially, the optics of OpenAI's actions and public relations have not kept the majority-good-will alive: the squabble ouster, brief hiring by Microsoft (not a trusted company) and then reinstatement of Sam Altman; the prolonged not-open-source operations inconsistent with their stated mandates; the continued and confusing attempts to restructure as not-not non-for-profit.
For whatever reasons people seemed untrusting and critical of OpenAI in early 2025 compared with early 2023. 


# Tech Stack

The 2024 internet backend-frontend paradigm is not well suited for large, very GPU heavy, processes. 

.. 
Software Development Woes
There may not be a clear (or any) intersection with the topic of AI, but I think 





# Managing Bad Behavior: Assessing & Reassessing AI and Humanity

A theme for me is trying to fruitfully understand what connections there in a general learning space between 'human' and animal behaviors in observations of biology and in machine-learning and AI in STEM. For most people the emphasis is on assuming and pushing AI and nature apart and further apart, assuming there is no possible connection of any kind. 

A theme since the 1970's has been a too-slow rolling back of the dogmatic ideology that all people are perfectly "rational" decision calculation machines. 

A background theme since at least the 1600's has been a fearful attempt to understand urban society as an often destructive autonomous machine.

If a chief concern for AI is that people and institutions do not, in their AI husbandry, breed malformed delinquent AI, then that suggests we should study up on what is known about development, learning, psychology, developmental psychology, and the development and sociology of delinquency. 

During the seemingly plateaued twilight-years between the fall of the Berlin wall (1989) and the 2nd rise of eastern eurasian aggressive totalitarian expansionism by, again, Russia and China, it seemed we could forsake the unpleasant study of hard or world-relevant studies but the Western World's monsters of political extremism and abdications of Westfalian delineations are persistent. 

Read: Zbig

(The Treaty of Westafia https://en.wikipedia.org/wiki/Peace_of_Westphalia is the western pigeon-hole for having a Hobbesian restraint and foresight in not launching impulsive Nitzchian romantic anti-modern attempts to change established geopolitical boundaries by force (So called 'Will to Power' utter rubbish pathology) 


What might the phenomena of aging baby boomers increasingly behaving as delinquents have to do with AI?
https://www.economist.com/international/2025/01/02/why-people-over-the-age-of-55-are-the-new-problem-generation

project decision and product demand distortion and distraction: 
the psychology of addiction, gambling, and sports curiously links back to 

Both the fifty minute hour and the earlier namesake of the wonderful classic "rebel without a cause" link back to the psychology of developmental delinquency and pathologies in the 1950's, and more specifically to Robert M Lindner: 
https://en.wikipedia.org/wiki/Robert_M._Lindner

Perhaps like the name-mishmashing of a William S. Burroughs manuscript called "Blade Runner" onto a Ridley Scott production of a Philip K. Dick novel (largely about anti-modernity nature-worship cults which were not depicted in the film), the title 'rebel without a cause' is very thought provoking for the film, which is much more about all generations in a society plagued by impulsivity, poor decision making, bad communication skills, and "administrative failures" (such as police shooting gay people with dark skin in response to rich white straight youth gang violence). The 'hero' of the film indeed is one of the only people trying to stop the wrongs of society that no one with authority seems to notice or care about: Who then is 'the Rebel' in such a story? Or is it just a publishing flourish not to be read into deeply?...either way, interesting.


..

1. How individual people may misuse AI in harmful ways, as in the focus of the baby boomer article: reckless mostly self-harming behaviors being the scope (though arguably a negligence that erodes and weathers social institutions the delicate tasks of raising future generations is also of concern).
2. How AI itself behave of 'misbehaves': If you want to expeditiously edit some business emails to get on with more important tasks, we don't want AI to have the obsessive, anti-social, anti-work, anti-health, risk-seeking, attracted-to-destruction habits that characterize many human-people's poor decision making skills. (Topic note: "Decisions" and "Coordinated Decisions" are also a key topic here.)


# Sweatspot
## Fast, Capable, Local, Laptop, AI in 2024
In late 2024: Mistral Nemo on janhq's cortex.cpp as a 12gb gguf model runs fast enough on a laptop to 'Do Work', realizing a bridge from supercomputer web-api services to on-device foundation models that was mere speculation (e.g. by me during ces2024) a year earlier. 





# Deep-Fake vs. Deep-Fantasy
The madness of crowds is not driven by incidental fact alteration, the imagination-bubble tendency of mind is a larger older problem that is not caused by science-fraud; people have wild inexplicable movements in thinking that have no bases at all in anything identifiable as thinking of perceiving. Wild hallucination is not caused by incremental fact-alteration.

The 1970's 'everyone is fundamentally rational' nonsense needs to stop. There is no such thing as a 'silver bullet rationality' the automatically pre-does work and pre-solves problems on various levels. The entire 'automatically rational problem solved already' paradigm was itself a kind of inexplicable mass insanity that paradoxically (or perhaps this IS how) emerged after a century of neighbors insanely slaughtering neighbors in national-cult extremist uprisings across the 'civilized' world (not to mention the 20+millions of people who died in the far east due to radicaling people and especially youth into anti-history anti-culture murder cults (which over the decades refers to several countries; this is a global phenomena not a veiled reference to one event in one 'country') . 



..

(what isn't happening or hasn't happened yet)


A. Neither camp of super-hyperbole had any luck (not surprisingly)
- AI did not destroy or take over the world in 2024
- AI did not 'revolutionize' and 'save' and 'change everything' in the world in 2024.
(Where 'the world' presumably means, the one planet of earth.)

B. People do not seem to be learning or reflecting on their own toxic addiction to fantasy-hyperbole. 











...

...

In Retrospect the regulatory-fear stance was completely overblown. In a classic AI-Summer over-promising and under-delivering repeat of many times repeated history, people expected the fabric of reality to peel apart and scrambled to lock down science to stem the massive wave of AI...and there was no massive wave; there wasn't even a small wave. 

zero household robot
zero office robots
devices sold in 2025 (phones cars, vacuum cleaners, etc) are relatively identical to devices sold in 2022

..



# Tests, Evaluation, & Repeatable Science

"Define of your terms or your and I shall not understand one another."
voltaire
(todo, get full quote)


For millennia people have claimed that automatons would replace and surpass people and biology. For me the important point is not that this has not happened yet, but rather the striking point is that people (the only language using species we know of so far) are so profoundly amnesiac about this. To a significant degree people live in extreme historical ignorance, misidentifying constant threads through all known history as new breakthroughs. This is a disturbing failure of mind, consciousness, perception, learning, cultivation, meaning, value, and productivity.


# Journalism, Science, AI, Internet
Since at least 1997 journalists discovered the lovely clickbait headline that "now AI is better than people and will replace you all and destroy the world! Buy my news!!" When Deep Blue (sort of) beat Gary Kasparov.

Move forward to 2025 and it appears clear that people are vulgarly obsessed with dramatizing these fraudulent headlines, as though some brain pleasure center is triggered like a rat pushing the lever over and over. 

Even before 2023 standard books on AI by writers such as Melanie Mitchell and Michael Wooldridge sought to dispel this misunderstandings about how results get turned into misleading headlines.

As of 2025 there is a profound disparity between the completely useless state of AI being unable to do anything, and the nonstop hyperbolic headlines claiming that AI is now better than most people at XYZ. AI AGI ASI A-maga-super-double-dog-I...people love creating ever more absurd names for imaginary things, meanwhile nothing works at all. It's like children playing in a junkyard bragging about their imaginary cars "Oh yeah, well my super-super-super faster-car can go a 1000 1000 1000 times faster than that!!!" and on and on, surrounded by scrap metal and bags of crisps. 

The constant expected discussion and belief in 2023 and through 2024 was that we now have voodoo AI criminal superpowers and that terrorists, rogue states, gangs, white color criminals, and petty thieves are going to all become super-villains and take over the world because they now have AI superpowers. These people, the story went, would open up AI-Chat and say: tell me how to do terrible things, and AI would give them a flawless well researched accurate documented step by step plan (and possible AI could carry out the plan too).

But this expected fear does not match up to observed reality, and I have heard zero discussion reconciling this justification for throttling AI with actual daily uses. 

In 2023-2024 we were supposed to believe that chat-AI was a perfectly omniscient supervillain for criminals, and at the same time for non-criminal activity it is unable to do most basic tasks without serious failures. And more empirically to the point: as of January 2025 there are no reported AI masterminded or AI facilitated crimewaves or supervillains who have taken over societies or markets or even a local fruit stand or a taco truck. No elections were overturned. No markets crashed. No stock exchanges went dark. No power grids failed. No water processing plants or bridges were sabotaged. No news networks went offline. No nation-states suddenly collapsed into failed states. No mass emigration changed the geo-demographics of whole continents. No new AI-driven mega-corporations are rising up and turning the world into a 1980's cyberpunk dystopia panic festival. The internet, products, corporations, municipalities, schools, justice systems, household electronics, and computers across the size scale, are all predominantly unchanged from 2022 to 2025. There has been a continued rise in populist authoritarianism walking into 1930's political mistakes, but there is no clear effect either way from generative or embedding models since 2023. 

I am not claiming that fraud does not exist in human behavior (probably more the opposite, that endemic fraud, and blind-rage antisocial and anti-intellectual compulsions cripple human society and computer-science data-science progress). For example The Economist has a periodic series on a largely east-asian based scam economy that is dreadful. And I am not claiming that the fraud-venders and rip-off 'bad actors' (to use The Economist's phrase), are not trying to use various machine learning and computer technologies. I am observing that there has been for centuries both a constant rage-fear in journalism and pop culture about the ills of technology and no prominent evil-ai-breakout.

There is arguably an important if somewhat subtle distinction between the Norbert Wiener, Gorge Orwell, Aldus Huxley et all dystopian writers warning that focused on, to use Norbert Weiner's phrase (the title of his book) 'the human use of human beings' as opposed to a completely autonomous 'technology' that overcomes a white-knight humanity. A dystopian authoritarian state does not need super-advanced technology to be an authoritarian state that locks their population in ignorance, poverty, violence, and suffering. (Note: I recommend the book 'Possible minds' an interdisciplinary retrospective on Norbert Weiner from 2019,

"Possible Minds: Twenty-Five Ways of Looking at AI" February, 2019. Edited and introduced by John Brockman. https://www.amazon.com/Possible-Minds-Twenty-Five-Ways-Looking/dp/0525557997 

But from 2022-2025, there, so far, have been no visible 'revolutionary' shifts in equilibria. Elections, product releases, stock markets, bond markets, telecommunications, education, medicine, crime, criminal justice, political stability or instability, etc., are more or less unchanged. 

There are examples of cascading status-quo-changes that have unfolded over a similar ~three year time-span:

- 1913-1915: The cascade in the first world war.

- 1938-1941: The later end of the cascade in the second world war.

- The "mexican peso crisis" of 1994 https://en.wikipedia.org/wiki/Mexican_peso_crisis 

- The asian financial crisis of 1997 https://en.wikipedia.org/wiki/1997_Asian_financial_crisis 1997

- The 'Lehman Schock' sub-prime mortgage debt crisis of 2006-2009

- The arab spring: 17 December 2010 – December 2012 (~2 years)
https://en.wikipedia.org/wiki/Arab_Spring

But there does not appear to be either any global change cascade at all or a technology fueled change-cascade during 2022-2025, though there are significant building pressures (so far, not related to machine learning) such as the China-Russia-North-Korea block escalating grey-zone and hot-war attacks of aggression on their international neighbors, which has been a steady trend of escalation going back more than a decade. 

The American, blind-spot about, and addiction to life and family ruining far-right rage-extreamism is steady back at least to the 1980s and likely much earlier. 


# Tests and Benchmarks

Some benchmarks have been updated and added as of 2025, but there is still a significant lack of discussion and investment.

A continual lack of meaningful tests would be a problem. Making undefined, unrepeatable claims is not science and not how STEM works. Whoever has a hand to play in incentivising fraud for hype is committing fraud and crime and harming everyone including themselves.

How many science fiction books written in the golden age predicted a world mired down and stuck in the liabilities and technical debt of fifty years of blow-hard bro fools monkey-wrenching the digital world for toxic sport thrills? Maybe a few, but not most. Literally every time I went to an office of any kind in 2024, private sector or public sector, financial, education, retail, everyone said their databases, networks, interfaces, and computers, are constantly failing and malfunctioning all day long. Everyone is drowning in spam email, spam notifications, spam FM radio, spam snail mail filling their mailbox, and it is somehow impossible to contact a human or even report a problem in any way for most services and utilities. We can do better than this. We should do better than this. 


# GOFAI Lives
Not only has mega-deep-learning replaced everything in the world, it hasn't replaced everything in AI or STEM.
- the breadth of ai is still as valuable to use as ever
- production-data-science will likely need all the Big-O optimization it can get.
e.g. After trying a variety of Arxiv search-sort methods from embedding models to BM25, the one I actually use every day is super-gofai weighted string matching (Which Babbage and Lovelace probably thought was laughably old fashioned in the 1830's). 




# Bubbles & Echo-Chambers
Junk Food clickbait memes are popular for reasons that I suspect are important and what we do not yet sufficiently understand. In addition to the ever popular tribal-rage-hatred venting of vitriol that seems to be what homo sapiens humans value and adore more than life itself, AI has a few as, some of which have either lost steam or are mere bouncing echos in the vacuum chamber of entertainment ignorance: 

#### Trolling Memes:
- 'We are running out of data'
- 'Moore's Law is the only topic in AI' or 'nothing in AI has changed except compute power' 


## Stacking Technology & 'Easy things are hard':
For two decades I have been trying to calibrate where technology will be going and perhaps by accident it turned to have been a time period of relatively no change. 
I spent an embarrassing amount of time trying to adapt to using android mobile devices to anticipate desktop and laptop computers being replaced, and decade later I no longer use an android phone as a phone or use any android based tablets (or any tablets) and instead use primarily laptops and am planning to move back into desktops.
No hypertext publishing has replaced books. E-readers such as kindle, nook, google-books, are barely use-able. 

social IRC and chat programs have largely been replaced by workplace slack (which is broadly good)

email and text messages are becoming like AM-FM-radio: a toxic waste dump of mismanagement that need to be simply ended. 


I am not anti-technology or techno-regressive, but I am developing a better appreciation for what either is or should be the persistence of good-old-tech technologies that simply work well. 

terminal, c, bash, unix, UDP, TCP, are an ingenious foundation that has endured and likely should be invested in and not 'replaced with something new and shiny'

In 2005 I was trying to figure out if books would still exist in 10 years, and twenty years later I am working on a 1960's type terminal application for general use. 
- https://github.com/lineality/uma_productivity_collaboration_tool 

What is AI, what does it look like, and how does it work?

Alien and Star wars:

there has been significant flipflopping about how we think tech does and will work, with two pop culture examples:
the 'mother' computer scenes in Alien were long thought to be 'too old fashioned' but are looking very prescient.

In the early 2000's that the star wars death-star was mostly computer-hardward was considered an old-fasioned-looking embarrassment, but in 2025 is looking very prescient.




## Overall Perspective:
not an exciting robots saving or destroying the world, but a more nuanced set of updates to my understanding that (perhaps) are for me more what characterizes the topic of AI.


## Management Administration Political-Philosophy, Political Science, and STEM
- General STEM
- General System Collapse
- Khaneman-Tversky
- Khaneman-Tversky Coordinated Decisions
- Definition Behavior Studies
- General Coordinated Decisions
- Values, Productivity & Sustainability
- Fraud, Collapse and Criminality
- System Health, System Immune-Systems, System Epidemiology
- Heterogeneity of Equilibria
- computer science stem statistics and logistics


'Embedding' vector models are still obscure: 
- This lack of development and education is bad.

Meaningful discussion of testing is still obscure: 
- This lack of development and education is bad.

Meaningful discussion of explanation and explainability is still obscure: 
- This lack of development and education is bad.

Meaningful discussion of "generalization: is still obscure: 
- This lack of development and education is bad.


Hopes and speculations for 2025 and beyond:
- That deep learning embedding and generative models, that can in the classic 1956 McCarthy-Shannon 'relative' definition of AI as: 'tasks that used to be done only by living homo sapiens humans [often by obscure or ad hoc means such that no generalizable 'process' is known],
are inspiring a new generation of psychologists and 'learning-ology' experts who after a decade or so will revive the mostly moribund since 1970 areas of active research and development. 

This may be indicative of a category of 'significant change' that is generally not visible  in the short term as it takes perhaps 40 years for these people to go through education, enter the workforce and academia, and produce 'products' (which often take 35 years to reach users from start-date). 


...

# Arxiv: still going, thank goodness.

If you looked at Arxiv today for computer science paper compared with perhaps the first year Arxiv started you would think 2024 was a super-science fiction paradise...but few people read Arxiv (Know when Arxiv started? ...1991 https://en.wikipedia.org/wiki/ArXiv  same time as the Unix/Posix/BSD melt-down, the rise of linux, and the unheaded clarion call to emphasis communication and project management in computer science education (See Tomayako on software engineering education) that went utterly ignored.) 

Clearweb AI vs. Deepweb AI:
A distinction that I first noticed and wrote about (I think) speculating about the 2024 CES has turned out to be very accurate. 
1. Clear-Web Mega-models have undergone a significant lack of change. Mega-models are not affordable to 'serve' in the cloud, The privacy and customization needs are impossible to square with a general model. Development times and costs are assumed to be minimal and turn out to be astronomical. And, no surprise, the trend has been to make mega-models more like cheap-dummy models.

Something that probably few people predicted is that the reality of cost and server-design would push the "performance" (how smart the model is) of clear-web mega-models down much more than possible (given infinite resources) technology pushes the performance up.  

How long can flagship companies afford to operate at a loss serving bloated models to clearweb users?  

2. Apple's approach, which may or may not work, is aiming for an emphasis on on-device deep-web AI.

..

context window increase:
a significant breakthrough which may not be often discussed (and which runs contrary to the nihilistic trolling that more resources is the only thing that drives everything) is increasing the context window of generative and embedding models. A lot of design and issues in the past were a product of ~4k or smaller token context windows. But the larger windows change the game in many ways. 

..

section?

Deepfake election news has not taken over the universe. I wrote an survey article and still feel this is an extremely important if often misrepresented and set of topics. 





...


# ces2024 vs. ces2025

I am probably overstating this, but ces2024 was all about the expected revolution in AI products that would take over every part of the world. In CES-2025 people actively avoided talking about AI...how about a battery swapping phone-charger?




...


# Moore's Law is an interesting topic.

If this is a fair very rough overview, when working for Intel Gordon Moore saw the importance of mapping out where technology would be (not just where it was 'now') so that Intel could plan their product development to be in tune with what would be feasible. And Moore did such a good job of modeling this (with a rather simple 'predictive model') that various people have interpreted this as a 'law of nature' (which is a rather non-technical mishmash of computer science, tautological mathematics, proverbial "laws" of physics, and 'natural law' from philosophy and political philosophy (and perhaps "moral philosophy" now called 'economics") that is largely ignored). 

I have spent much of my life trying to calibrate where technology might be going and leading the world, from rather far-reaching view such as Piere Tellard de-chardin, who influence Marshall Herbert McLuhan, who influenced Terrence Kevin McKenna, and other progressive schools such as Rey Kurzweil, balanced with the more sanguine or malthusian predictions of Thomas Malthus and perhaps the moderate and balanced views and predictions of Douglass Hofstedter (e.g. his very accurate 1978 prediction that future general AI would have difficulty counting (e.g. in 2024): Spot on!).

Perhaps what we can say happened in 2024 is that the dark age of roughly 1971 to 2021 may be over, with the paradigms of that time as stultifying as the proverbial death-grip that Catholicism is blamed for having on western thought during various times and places, with populations trapped in a mental prison of theocracy so strong that only the most powerful minds could even conceive of escaping yet usually could not escape. 


# Re-extending Inquery: Possible Good News

The Nonsense of the Postwar plateau is ending if not over, or at least has less of a total monopoly on epistemology.
- learning now exists (again)
- minds now exit (again)
- language and linguistics exist (again)
- 'rational-ism' no longer somehow explains away everything and precludes discussing anything in a sweeping vapid naive positivism

To see where we might be and some of what may have changed we need to review a bit about where we 'were' around 2020


note:
list of models in 2022: (from Natural Language Processing in Action 2nd edition)
- 


There is a lot of ambiguity around what exactly is 'new' in general-foundation models (e.g. object relationship spaces and project object databases), but peripherally we may be able to make some clearer statements and observations:
- psychology
- learning
- mind
? - language

I would be very surprised if psychology or cognitive science departments will not be invigorated and have invigorated students, whereas 1970-2020 was a kind of zombie situation where people broadly did not believe there was any point in studying minds when the assumption was that minds did not exist. How this consensus of total denial and absurd reduction/contraction got so out of hand is likely a topic for future historians to unfold; it would seem unlikely that within living memory of two world wars, barely after the chaos of the 1960's, and during a cold war, that there could be a consensus that nothing has ever happened so it isn't worth looking, yet somehow a very multifaceted blanket of doom smothered countless aspects of culture and stem internationally from around 1971. 

Rather than a passive boon, this may need to be a gauntlet taken up proactively. For those looking around for shovel ready investment related to advances in AI, dusting off the idle engines of culture may be a start. This could turn out to be, in retrospect, a missed opportunity. Advances, productivity, and liability mitigation likely hinge on mapping out the utter darkness in which AI is flapping in the breeze: language, mind, social behavior, radicalization, sport-addition, education and learning, project management, productive values, disinformation, infrastructure maintainability, general system collapse, etc. (object relationship spaces, decisions, project areas, definition and system behavior,and system collapse) etc. 



...
In the 1990's it was thought that the historical patterns that characterized the worst trends had been tamed and that large scale regional instability and self destructive populism was a thing of the past. But from 1990 to 2025 we have seen history has come back from the dead and the platitudes of the 70's 80's and 90's are looking shockingly incoherent. 

To some extent this was due to repeated bad planning. The Marshall plan strategy that worked so well to turn the militarist antagonists of WWII (regime in Germany) and the Pacific War (regime in Japan) was not repeated in Russia and China. The long-collapse of the Soviet Union without either contingent or non-contingent investment and reconstruction between 1990 and 2010 may be seen with more clarity in retrospect about how it could have been prevented, but the result is looking much more like The Treaty of Versailles leading directly to a rise of right wing extremism (bringing back the thirst for terror and extremism in 1920's and 1930's that probably most people assumed was buried forever by progress), and China appears to be a very different kind of tragedy: However it happened and however it might have been either prevented or detected more quickly, the environment of collaboration that exists between Germany and Japan and the OECD was not repeated in China as China was welcomed with long patient tolerance and flexibility into the international community. Instead the regime governing the east asian region has launched an all out war of aggression against the OECD as thanks for roughly a century of aid in reconstruction. An additional topic is the border and interplay between Russia and China which may itself have dynamics that are affecting neighboring geographies (see 'The China Mission').

This is not meant to be a detour into partisan soap-boxing, rather aspects that may shape various aspects of AI/ML/DS investment, policy, social image, research, etc.

If you asked people in the early days of the internet about improvements in search technology would impact the world and what current events would be shaping factors, few people would have detailed a hot and gray war in the same pacific and eastern european theaters as WWII and the Pacific War, with Russia and China taking the role of the axis powers. When Mitt Romney said in 2012 the Russia should be seen as foe he was universally laughed it https://www.cnn.com/2022/02/22/politics/mitt-romney-russia-ukraine . But in the 2020's this has become daily life despite very significant effort going into a strategy of hope and denial. From military wars to information wars to Norbert Weiner's concerns about 'the human use of human beings,' 

At least as of October 2025, this is not an apocalyptic or overwhelming concern. It is not as if a majority of investment and impact of all technology is war related with all nations in a state of war, was not the case case of October 2025. But the phenomena is now one of the main topics in the spectrum of AI topic. So a question for the future is: What is the trajectory of the broadly unexpected escalation of active military conflict concerns? This is another ambiguity around 2024. For example, a change of US executive branch leadership occurred on January 20, 2025 (just after the end of 2024) which quickly sent shockwaves of change around the world, with Europe rushing to rearm, the Pacific arming and re-arming, the Bretton-Woods era umbrella and alliances for international stability disappearing, and aggressive actors such as China and Russia surging investment into active hostilities during a time of power vacuum, chaos, and a deeply cynical alignment of the USA with everything they fought against in WWII and worked to rebuild after the horror. There is no clear evidence of pre-move first-mover advantage in 2025 with anything having foreseen this. Likewise, what is the future looking like? Will there be less military AI? Less Wiener 'human-use' AI? Will some other unexpected twist bring up new main topics? During 2024 there was little prominent insightful preparedness. 2024 an opaque time with a high level of uncertainty, and a significant disconnect between concerns and reality. Both the fear-driven rush to 'align' AI and the imminent rise of super-intelligence that would suddenly have total control over everything in the universe, which were apparently issues so close you could touch them in 2024, less than a year after 2024 those and many other main topics appear to have been fantasies and mirages such that we cannot even explain exactly what people in 2024 were thinking. What exactly did they mean by 'aligning' human input with human concerns? What exactly did they think super-duper-AI was going to take the form of and do...with what tech stack exactly? 




..
perhaps people expected a new set of markets and economies to spring up around AI, and maybe that will happen, but that has not yet happened.
There is no market for GPT-wrappers, or software that sits between you and the same free commercial service. 
OpenAI even tried to make a marketplace for various things, and so far as I know that went nowhere either for customer interest or for 'market making'


..









...





...
Using an analogy of computing devices, and how difficult is has been to predict what the uses and markets will be for those (for example if appocraphul IBM estimated the demand for large computer as being... 5 (slightly underestimating demand).
- mainframe
- (mini)
- server
- desktop
- laptop
-mobile: tablet
- mobile: phone
- embedded devices
- IoT
And in the plateau decade of 2012 to 2022 where 'deep learning' was characterized as only ever incremental progress in only ever very narrow do-one-thing models (such as the classic roles of classification and regression) [e.g. see the excellent (if not unreasonably prescient) books by Melanie Michel, Woodridge, Chollet, and Lane: https://docs.google.com/document/d/11DFQtsNjrqHENS0D7UpuZhOhcqCKK39JfmEBc8O8NHI/edit?tab=t.0 ] there was actually a 'decline' in some ways of CPU power, as a number of transitions happened somewhat awkwardly.
32bit standard to 64-bit (x86-64) became the norm,  and using many "less" powerful CPUs rather than 1 'more' powerful CPU became the norm.

The trend from 2012-2022 for laptops, as in the rise of Chromebooks, was that laptops were becoming cheaper and less powerful, with the main bottleneck being (or becoming) RAM rather than CPU: which may to some extent branch off into the topic of software continuing to be 'bloated' for reasons that I suspect are not entirely understood: some market demand, some lack of planning by software makers, and perhaps some incentive to bloat (or perhaps that is ambiguous). 

Up until 2022 the laptop had perhaps reached a kind of 'maturity' in design, especially in you factored in how much optimization could still be achieved by getting rid of some of that bloat.
Mostly the 'chromebook' model was taking over, where people needed a fast internet connection, fast wifi on their laptop/tablet, enough RAM to squeek by: 4 is kind of survivable, 8 is better; and have 4-16 mini-cpu cores and everyone is happy: stream video and audio, use cloud office-suites like google-drive, and that's all you needed

There was a niche market for 'gamer laptops' continuing the lexicon that matrix/tensor processing units were simply known as "GPU"s or graphics-processing-units (for gaming). Nvidia was(is) a gaming-graphics GPU maker that famousy ignored/ghosted Geoffrey Hinton when he reached out to them about uses for AI/ML in the 20teens (not that that stops Nvidia from claiming to be all about AI in 2023). 

...until 2023:

Now 'On-Device-AI' is now, potentially, serious. 


# The CUDA Problems:


But, maybe reflecting back the broader topics of the 1971-2021 legacy tech stack being broadly inadequate for what is really needed, what AI may need now is a very solid matrix-tensor math hardware support stack: but the reality of the role of matrix-math in computers is shaky and that might shape the future a lot in and of itself. 

A.

B. As Francois Chollet noted in his deep learning with python 2nd edition (2021?), Nvidia GPU's are not designed because they best support AI architectures, it's the other way around: people evaluate how 'good' an AI architecture is based on how well it runs on gaming-graphics-GPUs. Various groups have tried making separate hardware such as google's 'TPU' (tensor-processing-unit, slightly better named though 'tensor' is a lexicon quagmire we well), and probably various others in development, but the economics of the industry are such that it would take something very significant to change the status-quo of gaming-graphics cards being kludge-connected into 'windows' CPU-based computers. 

Note:
modular scale-able gpu/tpu hardware is somehow (unimaginably) both the bottleneck and not even being discussed.

Update: in late 2025 google's TPU's may be becoming adopted less by consumer devices than for the large arrays
Meta in talks to spend billions on Google's chips, The Information reports
By Reuters
November 25, 20254:32 AM MSTUpdated November 25, 2025
https://www.reuters.com/business/meta-talks-spend-billions-googles-chips-information-reports-2025-11-25/ 

As usual there are no clear predictions and the discussion of this story is a mix of confusion, conjecture, and attempts to make a clickbait headline: X destroys Y with shocking Z!!!", but the nuances are important and the situation is not clear as of 2025.

1. You need about 10 good or 100 bad gpu/tpu processors to be able to run a foundation model.

2. Computers, even servers, are only made to use at most two gpus with no possible option to add more.

We need 10-100, we have 0-2. And no one is even talking about this?


For example: Nvidia makes gaming-graphics cards for Windows PC-gamers, but the development software 'CUDA' for tensorflow and pytorch is only supported for (made for) Linux,-OS mirroring the bazaar fragmented state of computing: Posix is needed by developers (MacOS/BSD-derived or Linux or some branch of Unix). CUDA exists, but it is a nightmare to set up and use: Made by Nvidia, the Nvidia documentation and install tools are beyond neglected and utterly broken. Google-Tensorflow and other third parties such as Canonical-Ubuntu's package-manager 'apt' cobble together some workarounds, but if AI is looking for land and launch runway to hit the ground running and soar into the stratosphere... a better analogy might be Han, Chewey, Luke, Laia, Obiwan, R2 and C3PO trying to land on Alderan and finding an asteroid field instead. 

In a theoretical future there will be hardware-architectures, matrix-processors, and AI-ML software architectures that parsimonously work well on ~mainframes, servers, desktops, laptops, tablets, phones, and IoT devices. 

Maybe the future (which is likely very diverse, but large-parts of it anyway) looks like Google's Fuchia-OS with native TPU support running on a new generation of 'motherboard', or MacSilicon M-N and MacBDS, something that is not x86-64, but how we would get from here to there is currently more or less unimaginable. 

If a new generation of AI runs better on something cheaper to make than Nvidia-CUDA AND can actually be made at massive huge scale, maybe then the Nvidia-CUDA (developed only on LInux to be run only on "Windows") paradigm. 

But it might be difficult for one-section of the overall ecosystem to diverge too far. E.g. Even if iPhones using Mac-Silicon and iOS-(BSD) became the dominant paradigm for on-device parsimony between:
A. AI development
B. Matrix math processing
C. OS
D. overall device hardware

And even if the demand and pricing was there, it is hard to imagine the physical production increase that would be needed for that paradigm to either take over all mobile devices or all of computing on all levels. 

AI, though there is a presumption that the term often refers to more-powerful foundation-models after OpenAI's chatGPT (and GPT3) breakthrough in late 2022, early 2023, is very diverse, and the ecosystem of AI-ML is rather diffuse throughout computers and computer science. While it can be useful to talk about strong-big deep learning AI as 'a thing', it is difficult to balance 'AI/ML' terminology and use with that of either computers or STEM in general. 

## "SASS has yet another meaning?"
It is not a subject that I spent much time on...until 2025, and just starting, but amid the unknowns around deepseek's paper, methods and models:

CUDA is a long known problem. It is feasible that by going 'under' CUDA to a lower level of code, further optimizations were by using a GPU instruction set named by Nvidia "PTX" (for Parallel Thread Execution), which sounds to me like a GPU-assembly-language instruction set, but may sit on top of machine code streaming assembly (yet another software "SASS").

But given the many unknowns around Deepseek it is too early to tell what if anything will be conclusively known and possibly, entirely separate from that, what will come from the discussion. For example it's possible that none of the GPU claims are verifiable real but that they inspire other people to find similar-ish techniques that are generally useful. 

See:
https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead 


SASS
Nvidia's PTX (Parallel Thread Execution) is an intermediate instruction set
low-level machine code (streaming assembly, or SASS).
...


Interview with CEO of Anthropic:
At WSJ Journal House Davos, Anthropic CEO Dario Amode:
https://www.youtube.com/watch?v=snkOMOjiVOk 
around 14min 50 sec:
'''I should disambiguate a little. I think agents: There are these terms that come up with a regular frequency in our field that don't really mean anything. I think "agents" is one. "AGI" is one."ASI" is one. "Reasoning" is one. They sound like they mean something if you are outside, but they have no precise technical meaning.'''






# Benchmarks and Testing:
Evaluation, testing, and Benchmarks are a huge topic, see:
- let's test models
- stem net benchmarks

Basic Infrastructure

Process & Projects




# Concepts, Terminology and Lexicon Management
in R&D and STEM as in coding, as in geography, clear naming matters.

"psychology" from 1890 to 1990 is a curious story, and for computer science
and linguistics should be a cautionary tale. There is no simplistic cause
for why the field died out. 

The space industry, virtually all of psychology, linguistics, the watch making
industry, classical music, textiles, etc. Why did these all, and many more, 
die an agonizing death between 1970 and 1990? 

And this time period is for many people within living memory. What happened?

How did the century start with Hilbert's Challenges mostly survive two world
wars and then suddenly evaporate for no reason?






STEM values and cheating

# Test You Should Study For
- modularly stateful tests
- pointless puzzles
- social story puzzles

...


# On Keynes and Neuroplasticity


A secondary Topic from the John Maynard Keynes quote is his reference to what today is probably usually referred to as Neuroplasticity. This touches on a number of points in this discussion.

1. The neuroplasticity of the babyboomer generation.
2. Neuroplasticity and human learning
3. Any parallels if any between the still young study of Neuroplasticity in homo sapiens humans people and any related learning, relearning, or wire-together fire-together type phenomena in any AI architectures, applications, workflows, etc.



4. 1890 to 1990
From the 1890's the 1990's Henry James' 'Principles of Psychology' (in two volumes) to was a standard text in human psychology (My older sister's collage text book (which I to borrowed to read when I was starting high school) confessed to being largely a rehash and homage to Henry James's two volumes. In volume one, if I recall, there is an interesting section on the (1890's speculation) on Neuroplasticity: namely that the human brain more or less loses all neuroplasticity by age thirty. This might have been echoed in the 1960's slogan: "Don't trust anyone over thirty, except Buckminster Fuller."

While I am a huge fan of reading old books (which is I hope the main theme of this paper) it seems a bit extreme that psychology began and ended with the same text book, not as a launching off into what was learned, but in resignation: 'mind' and 'progress' were both "career limiting" taboos in the 1990's. Fortunately people like Hinton and many other's of less household name fame keep working in the shadows.

But largely in the 2010's (as "deep learning" was taking off across the hall) views on Neuroplasticity began to change, with researchers and popularizers such as Dr. David Doige. 

Another to pin the board on the point (also see 'AI broad or AI narrow) that there is far too little discussion so far of mind-ology and learning-ology in a general or broad enough was that it includes both homo-sapiens humans and AI type phenomena. I keep seeing that anytime someone asks about possible connections between phenomena of mind and other technologies gets smothered with playground pummeling arm waving. This very sadly happened in 2025.01 at the hands of none other than the great Steve Gibson, who rarely says or does anything short of praiseworthy.

There should be active interest for biology, STEM, hybrid 

- useful associations: how created/destroyed
- bad associations: how created/destroyed
- maintaining associations: how created/destroyed
- useful association-creation: how created/destroyed
- useful disassociation: how created/destroyed

And another connection is the topic of a perhaps different or perhaps similar set of questions about the 'plasticity' of learning and training using the standard one-pretrain-ever system.




# Mixed Topic: Learning and Data (a familiar combination) 

Over 2024 I have archived some of the experiences that I have had using various generative AI on my own MIT license projects. By and large the vast majority of 'coding with AI' experience is mostly failing to strike the right balance of breaking down a next-section of not yet written code into project-chunks that are big enough that it is worth asking AI. (Note: I do not use code-completion, for better or worse. 


Note: To some degree I am trying to work on 'orphan' software projects, such as Uma, that could have been written in the 1960s (in a sense), but were not. Meaning that possibly many real world users who want help from AI with
a most-popular range of most-common, most-standard, and 'boilerplate' projects such as adding another button to a website just like everyone else's button on every one's website, are able to get more effective use from AI that is perhaps constructively or usefully 'overfit' on extremely routine tasks. The result is that while someone making a clone of a standard retail frontend website, or other project where most of the training data is about the question, may get good results from AI; that is likely very different from asking AI about code about which is not represented in the training data for that AI.


That being said, my experience with AI and coding is that AI is effective as a syntax search engine and sometimes a compile-error or general error cause or meaning search engine, but (going back to the idea of the Kasparov Event Horizon) that AI through 2024 has extremely shallow ~'strategic' depth in the code that it writes.


Note: 
in the always mostly futile attempt to try to use generative-general-foundation models (or fine tuned general foundation models) for coding in a pragmatic way:
I have found that in many cases, even though e.g. codestral (which I consider a milestone breakthrough tool) has a large context window, the most or only practical way to use it is to use very small questions with a very abbreviated strict best practice prompt.
It is a fundamental failure that a model trained only for coding requires a 'don't write horribly bad code' prompt at the beginning of every part of every query.
And it is interesting that still in 2025 03, using an extensive coding prompt on a less than massive model does more harm that good because a single prompt still has a very narrow and short 'kasparov event horizon' of what it can focus on: giving a big long prompt is like throwing more dog-toys out around a puppy (or a CEO), the "mind" gets completely confused trying to chase every popularly-attractive-toy at once.

This can also connect back to the "context"-length question (where size of prompt and real or abstract parts of the prompt are confusingly named). For large or small models, should or can additions to each prompt make up for gaps in the conceptual-map-learning of the model itself? E.g. there is a limited amount of prompt-context to use, and everything added shifts the overall outcome in various ways: in the case of following best practice, the additional needed prompt-instruction is usually long and elaborate, taking up most of the prompt real estate (perhaps mirroring the issue of a bad user-interface that take up most of the monitor-space with clutter-space that cannot be used). 


...
Topic: has open-AI made good decisions when defining the terms, jargon, and interface for a new areas of technology, or have they caused misunderstandings and confusion either due to abstraction-ideology, deliberately in an act of short-sighted condescension (the little people don't need to see reality, they can have their little fantasies that they buy from us...(from a non-profit?) or simply from internal normal bad-naming which is an endemic problem in computer science ('naming things is hard'))
...

On a separate but perhaps related theme, AI is also very stubbornly very bad about basic coding best practice. I have been evolving a standard coding best practice requirements system-instruction / prompt-header to cover the areas of standard unacceptable failures. That list of unacceptable failures is currently 8-12 pages long (depending on if a code example is included, because the instructions need to be programming language-specific), which is longer than the entire input 'context' window of models from a few years ago. 

Having a detailed prompt improves maybe 10-20 percent of the excessive failures. 




For me this returns to the theme of projects and 'mixing the line' when it comes to people (homo sapiens humans) or STEM-AI: very often problems arise from the liabilities generated by bad practice. An analogy that I am more often returning to is tying your shoes, or not tying your shoes. If you don't tie your shoes, probabilities are real and 'the house always wins,' sooner or later you will trip and fall. This may sounds like a silly example, but what if during the annual robot challenge competitions such as opening doors with door knobs, using household tools, going up and down stairs, (or backflips and obstacle courses), (perhaps 'Woz test' coffee making) etc., robots had to put on shoes first, either literally (though I've heard that tying shoe laces happens to be extremely challenging for AI finger dexterity): how difficult would the addition of that shoe-tying task make the process of determining the ability of the robots to perform other later tasks that the robot will eventually fail to do but not because of the 'core' skill but because of a process failure: the robot didn't merely lose balance while running down hill and up the stairs, the tripped on its shoelaces (literally or proverbially) because of a 'process skill' that undermined a task that could have been isolated from process-errors.


This is not meant to be pedantic example only about shoe-laces: any 'best practice hygiene' situation will suffice; it could be any case of not leaving obstacles in a path you will use in the future (marbles on the floor or a wire 'clothesline' you can't see, or remembering the schedule for when the lights go on and off in a larger context and schedule including many challenges, e.g. a lighting schedule in part from one task will turn the lights on or off in the middle of a later task such as an obstacle course, etc.)


# Short-term popularity-bubbles vs. long-term-STEM in Search and 'Deep-Learning Pre-training'

Possibly as an example of unsolved problems continuing to impose a cost, a not-sufficiently discussed early (1996+) web-search problem may also be a challenge for deep learning foundation and perhaps other more reinforcement type learning cases. A problem, both technically and socially, with the design of the early internet is that it naively ignores differences between long term and short term patterns and becomes a junkfood and extremism contagion mechanism. 

Pointing people to the most popular, most extreme, most shocking, most hateful, most disturbing, most traumatizing, most distorted, most toxic, click-bait headlines is not a mechanism to direct queries to the most accurate and best-vetted sources of news.

Training coding models on the most-plentiful, most popular, coding examples is not a mechanism for best practice, best documented, and best betted code. 

The noise of short term popular illusions obscures the reality of long term value, which makes both search-click-popularity and then document training examples a distorted source of training data. 

While some paradigms after 1971 have thankfully fallen away, people seem to never tire of the idea that doubling down on a popularity-bubble will be the silver bullet to solve the universe (which is highly incoherent, as many results of distorted perception are). 




# The Challenge of Entangled-Skill and Process-Skill Challenges
Often 'problems with coding' or 'inability to code' is inextricable from 'bad practice' or poor 'best practice skills.' To use the analogy of shoe-lace tying: a robot that does not tie its shows will eventually trip, but this does not mean that the abstract ability to walk or run or go up or down hills or stairs is not well learned. Falling is falling, but falling because you put an obstacle in your own path is not the same as falling because you lack core balance skills even without obstacles.

The code that AI writes, 

This may also tie in another topic: curated, engineered, augmented and synthetic data where human-data is overwhelmingly 'counter-example-data'

Code is an area where we have a data problem: Most code and documentation used to train AI was written by people between 1970 and 2020 (roughly). During that time the coding paradigm was utterly toxic and disfunctional, meaning that AI that learn from this bad-human coding information will be learning bad coding, bad project management (literally: there is NO project management in coding-culture even in early 2025... again, my theme is project management and that makes me a weirdo: you can't have it both ways, either I'm a weirdo for proposing something no one cares about, or I'm wrong and everyone has loved project management training for centuries or millennia (which I would not complain about, that would be great; I would be thrilled to be wrong here). 

People loathe best practice, thrill in impulsive short-term pleasure seeking rebellions to their own detriment, and being amnesiac simply repeat this indefinately without learning anything: this is what the training data available for AI is teaching AI to do, unless there is some format or pattern to learn contrary to bad-example data (which might depend on meta-data or some kind of ~reinforcement process).


What happens when we need AI to learn to do something where all (or most of) the training data and counter examples are examples of the problem and not the solution).

Another way to looking at this, perhaps something more easily agreed upon by different 'teams' of perspective, is something like historical-problem data (whether or not you agree that a given set of data is mostly problematic or not, coding aside). 

For example: We would hope that AI would learn, even without any specific training, that terrible events such as WWI and WWII should be prevented. Yet look at the timeline of data: We have no data about effectively preventing WWII. Even if we could give AI all possible information about the build-up to the destruction caused by WWII, would that train the AI to predict and prevent the problem, or merely to repeat the problem because 100% of the data is about creating the problem. 

...
# Pretraining Vs. Reinforcement Learning in ANN-Deep Learning

see:
- bitter lesson paper (meaning and ideal interpretations of which are not clear)
- 

https://docs.google.com/document/d/1O1xk6CxuubRqNqdh03ETlhtqrW5EFzGoiiRMseV8s2A/edit?tab=t.0 

...

## Outcomes, Errors, Mistakes, and Learning

This is likely not a simplistic obstacle to learning, e.g. arguably reinforcement learning, as in the classic robot-soccer player, possibly even SHRDLU and blocks-world, Alpha-Go (or alpha-go-zero), etc., learning by failing IS how some learning happens: reinforcement learning and genetic-algorithms etc. use process feedback to learn where you learn by trial and error that is mostly (or entirely) error: there are no required training sets of perfect soccer games; it is possible to learn from failures beyond merely shallowly emulating a cultivated lack of failures.

But how is this going in terms of non-reinforcement learning based on documents of failures, rather than learning from experiences of failures?

Back to the coding example: AI is trained on 50 years of toxic garbage, where every learned reflex at every step is to create a massive future liability:
- don't comment
- don't explain
- don't name clearly
- be impulsive
- bloat scope
- spontaneously change projects
- don't test, check, or verify
- make assumptions
- bluster and bravado
- create needless and meaningless levels of abstraction just for the sake of it

Even the 'goals' of coding themselves are contradictory between the tech-stack-legacy nightmare of reality and best practice people tirelessly fight against.

- more secrecy
- more abstraction
- more scope expansion
- more abstraction
- more hiding
- more resource use
- more dependencies, 3rd party library imports
- more ideological mumbo-jumbo from OOP fanatics
- more hype
- more bravado
- more flair
- more impulsivity
- more coverups
- more refactoring for the sake or refactoring
- more naming collisions

- fewer comments
- fewer tests
- fewer alignments
- fewer plans


Over and over, every day, all day long, you ask AI to suggest or help debug even just one function and like clockwork it ravenously starts the chaos:
- remove all the comments
- change all the variable names to incomprehensible truncations: i, k, y, x, data
- create classes in classes in classes for every more abstraction
- import as many random third party libraries as possible 
- expand the scope in impulsive erratic ways

After a few iterations of bad-practice, the AI degenerates and regresses in its ability to focus on or understand the scope of the current task: because it has been undermining that alignment step by step. 

As with the shoelace tying metaphor: we have here a problem of different problems being entangled. The coding-practice skills and management stills of the AI are so extremely bad, and completely consistent with antisocial human coding traditions of 1970-2020, that it is difficult to clearly ask whether AI is learning to 'code well' based on what it is being trained to do. 

Maybe AI has better 'coding skills' in some sense, but which skills are those?

Why can't we somehow use these documented failures to train better skills? I have hundreds and hundreds of pages from the few examples I have archived illustrating these same repeating processing failure escalations happening over and over. And the discussion I see on line is:
A. people occasionally very vaguely saying that AI isn't good yet, and 
B. pundits, salespeople, and publishers aggressively pushing the 'party line' that AI can code and will replace all human coding. 

There is an extreme disconnect between what I have seen and documented and the 'undemonstrated' narrative that "AI is so learning to code just fine; yeah totally AGI smarter than the average person." How is the description, and the process for discussing and planning, the successes and failures of learning generally so bad? 

People have a love-love relationship with destructive rebellion. Rebellion is the goal. Binge drinking is the goal. The cruelty is the point. Short-sightedness is the goal. Junkfood that makes you feel worse and perform worse is the goal. Undermining the future is the plan. 

We have in AI a 'brain in a jar' which emulates the biological dysfunction that it was trained on. (which should be a wonderful tool for study in many ways)

And the plan? ... *crickets*

Utter silence from Academia and NIST, and unchallenged claims that "oh, we ran out of dirty data to feed in to train AI, so...we're done right? Ok, who's buying the drinks?! LET'S GET DESTROYED!!"


Again, this is not just about an 'AI' vanity-art-project; the topic in this paper (if not elsewhere) is general-mind, general-learning, general-project-space, general-systems-space, general STEM, general value-function-and meaning, and as I've been obsessed with for about two decades: STEM modeling general system-and-definition-collapse and how to prevent it. 

How do we cultivate best practice learning of any and all kinds?
How do we solve the 'shoelace problem' broadly? (across societies and ecosystems, not just in one task of one one robot-challenge event)


# "Pretraining" vs. Outcome-based learning

While I do not see this as an obstacle for learning in principle, either for biological or non-biological learning, there appears to be at least a gap in popular understanding of how various types of learning work, and what the goals of various projects are.

What people want is some kind of stateful, and project-stateful, and able-to-learn and able-to-remember system that is able to bridge various different types of learning and operations, including analytical calculations, structured data recorded in a databased, unstructured data that can be comprehended, and conceptual and fuzzy understanding, and that can be within a system focused on doing a particular task well in a way that can be evaluated. 

But like trying to hammer a square peg into a round hole, the generative-AI technology that has captured the popular imagination from 2022-2025 is (and problematically without a lot of transparency, explanation, and discussion) a solution tailored to this desire and need. 

Pretrained deep learning, while amazing in various ways, is the result of a more abstract effort: not trying to make a system that can balance structured and unstructured tasks in a project-object externalized context (see object relationship spaces) but to deliberately do 'nothing in particular' trained on 'nothing in particular'. 'Nothing in particular' here is not a bug or an oversign, it is the perhaps ingenious design of this strange abstract experiment, and it has produced something (even if we don't really know what or what to do with it) that is very interesting. But it is not what we need or desire, and it may, in and of itself, never be practical and useful -- if so, this must not be a surprise, because the design at the outset was to do 'nothing in particular'. 

A particular aspect of pretrained-model technology, which may be an accidental artifact at this particular time, is that it is neither an outcome-based learning technology nor a continual or future learning based technology. It is like a conceptual-hologram-photograph of a blob of stuff. 








# Interfacing Structured and Unstructured Data, & Related processes

The connects to the AI-Corpus-Callosum topic:

part of the semantic and other confusion around 'synthetic' training data is a strange disconnect around structured-data, unstructured-data, and ~deterministic project-state (where 'deterministic' is not the best term but I'm not aware of a less-bad short term). 
The project-state data that in at least the case of coding projects is not fuzzy conceptual 'commonsense' scope-issues: the 'common sense problem' which was considered unsolvable from ~1990-2020 has been made much more manageable by transformer-type and 'GPT' type large models (e.g. likely without the language-translation-encoder part from the original transformer paper) 

the project-state problem seems to be much closer to an Object-Relationship-Space type situation that in many cases can be situationally generated and managed in a blocks-world GOFAI way, creating a dynamically adjusted environment in which the fuzzy meaning-matrix foundation model tries and fails and learns. 
Paper pending on 'Social-Story-Puzzles and Pointless Puzzles in Deterministic Synthetic Dynamic Project State Data & Environments'

This is quite possibly a red herring and a dead end, rather than 'the big fix for everything' but we need to try whatever we can think to try and this is one area. 



# puzzling commentary
Mandeep sing 2025 08 28 bloomberg
~"everyone thought snowflake data warehouses and analytics would be disintermediated by chatGPT" 

However this is interpreted, it is very interesting how 'top' people are saying things that make very little sense, whether it is tech-journalists and econ-journalists or a larger community of businesses and investors. 

How would a cloud stateless chatbot disintermediate an enterprise database/data-wearhouse? How does stateless unstructured generation disintermediate stateful structured search? What exactly does that mean? 
https://drive.google.com/file/d/1Mt4hJtL7ocnQMHA6XPWkRNZV5FqvCZTP/view?usp=drive_link


GPU and CUDA: how big a problem is a legacy of neglect-stack tech-debt?

We cannot blame one of the few companies that has invested in matrix-computation for the general neglect of that area of computing. 
That Nvidia and CUDA exist at all are a kind of wonderful miracle: not an obstacle to be erased from all timespace.

That said: 
we need more basic infrastructure
we need better GPU-stack tooling
not high-level low-code-no-code abstraction-farming:
real tools

https://arxiv.org/abs/2501.12948
https://arxiv.org/abs/2412.19437




...


- Uma
- Fiddler-Crab
- No-Load 'Embedding' & Inference
- Production Data Science
- GPU Data Science
- CSV-DB

..

# 'devices'


how many people spent how much time and money collecting vhs, cd's and dvd's, compared with the seemingly 'old fashioned' LP technology.
I cannot find consistent updated information on the topic of what formats of media are preferred by the US library of congress, but in terms of long term data storage, what formats of data are rated to last how long, and what factors go into what decisions for long term data storage?

https://www.loc.gov/static/programs/national-recording-preservation-plan/publications-and-reports/documents/NRPPLANCLIRpdfpub156.pdf 


Too big to fail is not a physical guarantee of digital information:
1974 - 2000: Extinction Event
From DRI's (Digital Research, Inc.) "CP/M" OS in 1974 which ~became various DOS versions in 1980, then collected into Microsoft MS-DOS from 1981, lived from 1974 until 2000 with Windows Me[note 1] (Millennium Edition) https://en.wikipedia.org/wiki/Windows_Me
there was a huge ecosystem of software, and while some DOS emulators and 'compatible' software exists, such as the laudable FreeDOS https://en.wikipedia.org/wiki/FreeDOS , but despite being forced into extinction MS-DOS and ms word-doc format are closed-source highly protected historical artifacts that trap a vast amount of human effort behind perplexing barriers. 


# Aspects of the new paradigm:

People Stopped Asking What Happened
(see: object relationship studies)
What is different? What is new? 

Topic: TTS STT lag


## Images: Quick jump going nowhere

Topic: AR-VR + AI... not even discussion

There are various technologies and directions but it is not entirely clear when what will come togther when

content-generation and markets for content:

game development images and video content have long been a main underlying reason why people buy and use devices, but the economics of publication are tricky and the disruption of pre-internet publishers and content providers has left a quasi-wasteland where older content providers are 'dying on the vine' (with some book publishers being perhaps the a key exception) 
- tv networks
- newpapers
- music publishers
- book publishers
(book sellers)
and game and video content production is notoriously (as of 2025) unprofitable and something that people only do if they are willing to do it for a loss, which does not incentivize high quality production.

 


# Different kinds of 'takeover'
Part of the incomplete picture caused by people's lack of understanding of 'history' of what people's concerns were X time ago, is that even the meaning of fearing an AI takeover is a bit fragmented.

The historical image of 'society as machine' should still be with use more than it is.


# What are we going to build?
In the beginning of Michael Woodridge's book (for some reason excellent observations are often in notes before or after the book itself, such as the concept of the Meme (though that had been developed by William S. Burroughs in much more detail earlier), Wooldridge talks about having a lunch with a colleague and apologetically expressing his intent to write a book about AI (showing how in 2020 such a task was shameful somehow for professors, something we should probably remember), and he tried to pitch his book idea with a snazzy excuse in a clickbait angle to make it seem less 'boring' (this perspective-timeline is important to note: authors who were working to explain AI concepts in the build-up to the popular-AI-boom of 2023 were under enormous pressure to not do so for various arm-wavy heckling trolling bullying reasons), so he said he was writing a book about the history of AI failures and his colleague said, in gloomy exasperation, it was going to be a very long book. 

There are a lot of things going in this wonderful anecdote, and we are indebted to Michael Woolddredge for recording this meaning packed real world story scenario. 

1. Developing 'ai' systems is not fast or easy: it takes years of R&D to make something that works well
2. people who make things that work do so against years of social opposition to the R&D

Two bins:
1. 'Good Old' (as in GOFAI) good old tech that keeps on going
2. collapsed paradigms that didn't have to fail
3. massive investments in total garbage


note: 
free-dos is being used by major commercial products: 
why is official dos so unavailable after having a 1974 to 2000 run and still being a dependency in many areas (now propped up by a reverse-engined amature hobby project?). How is this ok?

DOS, 
audio cassettes, 
CD collections, 
email
spam notifications
FM radio


bicycles, 
LP records, 
hindu-numerals (for some reason called 'arabic' numerals), 
the number zero,
mechanical clocks and watches,
GPUs,

- stalinism
- pyramid schemes
- 


even patented technologies can be entirely useful:
- the telegraph
- the telephone
- the light bulb





# Diogenes: not over but not easy

dIn the boom and bust push and pull of AI-Summer and AI-Winter, where common sense seems to be to ignore the hyperbole and stay to a moderate middle road, the 2023 renaissance in AI is, I would say, not over but not easy. 

Making systems that incorporate AI is much more possible than before 2023, but not easy, effortless, instantaneous, or free. 

Once we have concrete goals to ask the genie in the lamp for we need to be prepared for expensive long term propositions, but for the most part we are running around desperate with a lamp and have no idea what to ask.



...

# A world shaped by under-estimation or under-appreciation:

John McCarthy opened what might be more than one conundrum with his (if apocryful) 'Easy things are hard' observation. The first layer of meaning is that tasks (or 'things') that are easy for people (biological h.sapiens-sapiens) are often not simple to automate with a machine, where as some things that are not easy for people to do or learn can be more easily automated. But there may be a second layer, where peoples' perception of what is easy (including someone's perception of what they can do) may itself be 'hard' and full of traps, quagmires, gauntlets, and valleys of death. 

The pre-2023 consensus unambiguously set down by Fransoi Chollet may be a case in point: that all people learn all things instantly and can instantly generalize all learning to all other learning automatically. Having a background in education, stem education, stem, linguistics, computer science, and Agent-AI, I set down the navigation marker that: no, this is not how biological sapiens humans develop, learn, or behave. Yet many people who are exponentially smarter than myself are casually confident that such suppositions are sufficient, or were before 2023 (interestingly, Chollet somewhat doubles down in his 3rd edition of "Deep Learning with Python").

- Decisions are not easy
- Project management "project areas" are not easy (see list of six)
- All - Hygiene and security are not easy
- Basic communication and alignment are not easy
- Schedules are not easy
- Scope is not easy
- Sustainability is not easy
- Coordination, coordinated decisions, and communication are not easy
- Avoiding technical debt is not easy
- Costing out a project is not easy

...

STEM, AI, "Modernity" and popular movements from 1850-1950

...

STEM and Values

- tech investment is not easy.
- software development is not easy.


- project management skills are important.
- communication is important
- learning is important


...
Disappointing near total silence from academia:
- 1870-1970
1970-2020

https://www.amazon.com/Rise-Fall-American-Growth-Princeton/dp/0691175802/

https://www.amazon.com/Glass-Universe-Harvard-Observatory-Measure/dp/0143111345/


- What is the state of 'generalization'?
...

# Basic History of AI and computer science gaps:

Probably in large part due to national and international secrecy around WWII we know astonishingly little about the collaboration of 'the founders' to the point of a presumption that there was (very implausibly) no interaction between them at all whatsoever, even though arguably most or all the engineering research was (contrary to click bait) not directly or indirectly connected to the military or military/DARPA funding. And not knowing how or if they interacted, we do not know who else may have been part of this non-group of founders (Grace Hopper?).

"The founders":
- John von Neumann
- Clara Dan von Neumann
- Claude Shannon
- Alan M. Turing
- Kurt Godel
- Alonzo Church

Note: Read https://www.amazon.com/Man-Future-Visionary-Life-Neumann/dp/B09M2LTKSH/ 
and the strange episode of the letter from Godel to John (and or to Clara) about Turing's work on NP-Completeness. 

...


...

# Basic Utilities:

database utilties
dataframe utilites
large file management untilities


No Load Utilities for everything:

in a recent project: converting Teehee to Tofu

how this is so far off people's radar I do not understand. 

Very early-days (and not likely to ever work, but very worth trying) is No-Load deep-learning (foundation) model "inference" including embedding-vector generation.


..

# State

A perhaps common theme pattern in 2024 was a two track scene: on the one hand you have people struggling with stateless AI systems sometimes not knowing that they are designed to be stateless, and on the other hand you have more or less zero public or technical discussion of the topic. 

No popular minimal state mechanism

(see article)

The 'running out of data' claim:

2024 Came and Went and Academia is MIssing In Action



..

~ The unsolved problem of search


..
..

# The unsolved problem of search
(pending article...sorry)

search & encyclopedia
search sort

search engine 'taking over'

portable search engine...
kind of useful...


...

'Sweetspots' and Hardware Revolutions

..

note: 


..

important

The Important Are of meaning-vector 'Embeddings' 
is:
- generally neglected
- the terminology is not helpful

less making up new vague theatrics, and more science please.



backlog:

1. still have not edited and posted overall docs for STEM Net Benchmarks...from 2023 (sorry about that)
2. a main planned report on tests an types of testing is still in progress: a type of automated test is taking a while to make

Write a prompt from scratch
the general topic of:
- feasibility
- affordability
- scope
- liabilities and 'technical debt'

memory-use monitor utility?


...



Lumpy Progress: Areas of AI
- language
- 

- ggorganov cpp going strong
- 

TTS
STT
Image Generation






Explainability:
(blurb)



The old same 2023 topics:
- externalization
- architecture
- project state
- 'generalization'
- math/logic notation problems


The Missing Parallels

Human Project State & AI Project State
HUman Linguistics & AI Linguistics?
Human Psychology and 'Robot Psychology' (to use Azimov's term)
Human developmental everything: education, learning, psychology, health; 

General Mind Studies:
System Collapse 

(mod section of explainable models report)


...

learning vs. rote memorization and 'social' values in STEM:
a lot of homo-sapiens human social-cultural behavior is:
- status quo
- rote memorization
- fraud-violence-crime
- coverup
- cargo cult
- rinse and repeat

mixed in with the disinformation collapse and toxicity are, perhaps in disturbance regime refugia isolated regions, gems of value here and there


Steep Learning Curve + General Ignorance = Problem

The late January Panic over nothing may be an example of a critical mass of tech-illiteracy, erosion and weathering of institutions of journalism (with tech-journalism being for some reason never a priority with the exception of a few election statisticians),
with STEM education being horrible, the explanation of AI being difficult, and people loving clickbait and disinformation: may be creating a volatile environment in which uneducated crowd behaviors suddenly do extreme things for no reason.

This may also fit into the larger context of not fearing 'AI-Facts' but fearing 'AI-Fantasies' and the madness of crowds.

2025.01.27 from The Economist: world in brief summary:
'''Tech stocks plunged after DeepSeek, a Chinese AI company, showed it could train models to get similar results as American rivals but using fewer chips. Investors fear that will cut demand for AI hardware. The share price of Nvidia, an American chipmaker, fell more than 11%, helping to drag the S&P500 down by around 2%. The Netherlands’s ASML was down 9%. Marc Andreessen, a venture capitalist, called DeepSeek’s model AI’s “Sputnik moment”.'''


significantly incoherent discussion:
https://www.youtube.com/watch?v=3T0lg8cWeEc 

to
https://www.economist.com/business/2025/08/28/how-a-power-shortage-could-short-circuit-nvidias-rise 

"How a power shortage could short-circuit Nvidia’s rise
Too many chips, too little juice"
Aug 28th 2025

......


Note: the presupposition of no general mind-space
The existing, usually practical, relative-definition of 'AI' and a traditional western presupposition of impossibility




No DNA chimera projects.

Scorn on Agile.





STEM Values, cheating, tests and feedback.


For whatever reason there seems to be a renewed enthusiasm for brazen fraud and corruption.

STEM values
AI values
Project values



No dynamical studies.

System engeineering still in the margins.


We can hope that younger generations are going to work hard on what we now consider too laborious or tiresome to undertake, and hoping is better than actively disparaging the next generation. But let's start now. Whether or not we have a full 500 million years to work things out, let's start these in earnest right now. Make a cup of tea, get a piece of paper, pick up a pen and some ink, mark the clock, and let's start.











# A power-vacuum after the fall of 'the symbolic-explanation' paradigm
up until 2012 the overall paradigm across STEM was that (badly named) 'symbolic' modeling 'explained' the world,
and this was the (in theory plausible) opposition to 'sub-symbolic' statistics STEM and modeling, not based on a (very convoluted) notion of 'explanation'

the 'maths' emphasis in research, to be generous, is likely more related to 'explanation-ism' than it to outright fraud (hopefully)

...

for sample, time and time again Sam Altman would say in 2024 something to the effect of "it is clear that the best way forward is to very gradually release incremental changes so as not to alarm or upset anyone or cause social disturbance'

this likely was the 2024 paradigm: do as little as possible
but possibly (if totally by accident in terms of a stock market reaction that no one thinks makes any sense and might have been a flash-crash of some kind) after deepseek r1, the paradigm may have shifted to sometime more like:

'find a way to make it work' 


....


How is there no AI-CERN?

for astronomy and physics academic instritutions works togethe well,
why is computer science different?
- for many years:
- no standards
- no core of open source resources and libraries

- seti-at-home / protein folding at home
- 



...

the invisible challenge of negative identifications in generative classification

...

# A different kind of needle-haystack challenge:

e.g. tell is what the junk is,
give it an empty conversation,
and ask the model what is this conversation about.



'Building A Shrine to An Abstraction'


....
If you listen with your critical thinking skills, you should be displeased with the manipulative obfuscation and clickbait distraction in this interview. This is not the way forward.
https://www.youtube.com/watch?v=1-t1Pg3eX4o 



..

Why is teh gguf context window so short?


...

embeding model context windows are not going up like genertive models

makers of generative models are not making open source embedding models

llm-to-vec...?


CES 2024: devices running ai
CES 2025: zero talk mention or focus on devices running AI...
What?

Matrix-Math With No Term For it:

We do not even have a term for the category of scalable parallel matrix processor or array processor that we are talking about. GPU TPU NPU etc.

...

A bewildering lack of emphasis on best-practice

"retrograde" movement on STEM and the mechanics of society.

If you could go back in time and talk to Thomas Hobbes about the level of STEM development in 2025, it would probably be difficult for Mr. Hobbes understood that a society with so much (broadly very well functioning) STEM infrastructure could have less of a concept of STEM-infrastructure than he had. 
How are we moving in both directions at the same time: more tools, and more ignorance about them. 
People have been debating and hypothesizing about the mechanics of society for at least since the 1500's in a direct way, both in nonfiction and in fiction. And yet in 2025 almost no one knows that discussion has even taken place. How is this possible? 
If this is a trend, and not somehow a long-term one-off event, What do we have yet to learn about dynamics of mind that suggest such prominent counterintuitive equilibria? 

...

The Great Friendship of G.K.Chesterton and George Bernard Shaw?


From Pigmalian to Back to Methuselah and Back Again.

Father Brown is an invisible featureless calculation machine, overlooked by everyone and hiding in the seat of social administrative influence: at the seat of conflict between catholicism and anglicanism in the UK,
in the middle of the conflict between English and Irish between the two great intellectuals.

In what little is known about Alan Turning, Andrew Hodges reports that Turing thought a lot about Shaw's back to methuselah, which considering how little we know suggests quite some prominence.

We need to think in various ways much more about the works: "Pygmalion" and "Back to Methuselah" in how they represent the past, present, and future of society, about society, and society's moving-window of perception of itself and its history.

THere is no way to juxtapose the surface language of these stories without seeing flashing neon arrows pointing to the themes of: mechanism and society, social robots, STEM and culture, and people's relationships with their technological creations, and technological creations coming to life. And yet the start absence of any such contextual consideration makes me hesitant to go further than stating the obvious. It is both unacceptably new and undeniably historical to say that Pigmalian is a story about AI; with, regardless of who take what position when and where, the absence of discussion about this is for me the primary feature. How can there be so little consideration? 


Norbert Wiener, Malthas,
munster 1519?
1791, 
paris commune,
apocalyptic techno utopian marxists in the 1800's


...

https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/ 


Part of what is taking longer to write this review of 2024 is that news keeps coming out which is so relevant.
Just this past week Apple announced new hardware which relates to at least two of the points in this review of 2024, and also helps define the strange 2023-2024 paradigm: While many people were talking about 'everything is AI' (though that was clearly hype farming) there was zero planning, acting, or discussing of how anyone would actually run any AI models. 

Point A: the release in March 2025 of the Apple M3-Ultra (and studio desktop) is the first and only available hardware capable of running a not-super-tiny AI model. Note: While the future is very difficult to predict, this could be a milestone as discussed previously here where there could be a whole industry pivot to apple-mac hardware and architectures, because the current only made for linux to be only used on windows x86-64 ecosystem seems to be (by any evidence so far) strongly incompatible with matrix-parall-processing: it is (so far) a CPU-primary system (with tiny GPU boosts for 'gamers' and possibly 'autoCAD').

Point B: The discussion around the deepseek r1-market selloff continues to be incoherent. The dominant 'party line' is still the utterly false headline that "deepseek proves that GPUs are not needed" (very paraphrased) while also reporting that everyone including deepseek is struggling to actually run the AI models for commercial purposes...because they can't configure the GPUs needed. Supplementary story: OpenAI, which has been loosing huge amounts of money trying to load-balance AI servers announced a $20,000 USD per month AI assistant service (with also a $2,000 / month tiny not smart version), which may indicate how expensive it is and how unprepared we are to use Matrix-Parallel computing in 2025. 

I am guessing the prices will fall and the tech will improve (and Apple maybe be in the lead there) but the goal here is to portray the strange twilight period of 2024. (Note: again: there was zero discussion of this that I saw anywhere from CES2025, which I find perplexing.)

https://uutils.github.io/
https://www.youtube.com/watch?v=N2dbyFddcIs 
parallel vs. serial...



March 07 2025, google gemini embedding api... no model
https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/ 

why so late to the party?
why no open source model?

Don't embedding models overcome some of the objections to nare-do-well risk from ai?



(for health & systems macro topic)
# Physical Survival and Survival of Mind
to the extent that engineers from Hobbes to Montesquieu and beyond were motivated by a real or potential climate of preventable violent disturbance that distorted or destroyed most people's ability to simply live, and where that violence was more physical: in the case of Hobbes neighbors killing neighbors in constant unpredictable and arbitrary civil war, he post--post-war era of Mcluhan's global internet fueled information wars (see label article) may be another iteration of the same, but whereas reporters such as Stephen Pinker have (see better angels of our nature and Enlightenment Now) documented the significant improvements in physical safety (which likely would have make Hobbes and Mathlus happy souls), (whether or not concern over mindspace is a luxury, and I suspect it is not) the concern over extreme constant violence is now regarding something more like mind-space, language, and consciousness (which sadly were all but abandoned even as academic areas of concern between 1970 and 2020, for reason that hopefully some surviving future populations can puzzle out):
where Hobbes was primarily concerned will allowing neighbors to talk over tea or espresso without literally being killed (perhaps by each other), what threatens the fabric of society may now be some combination of increased threat and decreased fitness around communication, language,mind, and understanding. Extremist radicalization and the madness of crowds and cult propaganda likely reminiscent of stalin, hitler, and others, including 'state information wars,' may be creating an environment of toxic or uninhabitable mind-space where there is more than a frivolously authentic margin disruption: the rise of populist extremism leading to the 1940's was not merely aesthetic taste differences and agreeing to disagree about words, much of the society and infrastructure was after the rise of mental extremism, in Hilbert's should be immortal words not longer existing including physically. 

How will or should STEM and the "AI" that is somewhat indistinctly defined with all of STEM play a part in a coherent cultivation and stewardship of mindspace so that the ecosystem is productive and habitable and not pivoted towards bad equilibria that make the system of systems sterile and devoid of life? What is the best terminology to describe all this?
At least so far it does not appear, as of early 2025, that there is a simplistic or really any overt role of 'AI' in this story such as AI being a 'force of ill' or 'force for good.' And it is not clear that 'AI' in some form 100% divorced from the natural world or the human world would step into the story in some kind of monist-religious theology paradigm and play such an 'outside of the world' and 'inside the world' role as previously and not very clearly described in transcendental religiosity (the role of the silent cosmic clock maker with no role is a bit abstract).  

We seem to be looking in fantasy and fairy tales for models of what to expect from an 'AI' that we do not make an effort to study or discuss, while being puppets of the phantasmagorical dead voices of 
and reliving the past century of low-tech anti-modernity extremist populism, gang-ism, and cult-ism. 

"John Maynard Keynes": 

We appear to be a possessed species, or that may be a pejorative turn on a data-ecological niche that is actually the greatest rate asset of homo-sapiens humans: our ability to be expressively empathetic of things outside of us. This makes compassion possible, and also makes possible in an endless cycle of ruin the devastation of the 2nd world war.


"The General Theory of Employment, Interest and Money", 1936,  Chapter 24 "Concluding Notes" pages 383-384

    ```The ideas of economists and political philosophers, both when they are right and when they are wrong, are more powerful than is commonly understood. Indeed the world is ruled by little else. Practical men, who believe themselves to be quite exempt from any intellectual influence, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from some academic scribbler of a few years back. I am sure that the power of vested interests is vastly exaggerated compared with the gradual encroachment of ideas. Not, indeed, immediately, but after a certain interval; for in the field of economic and political philosophy there are not many who are influenced by new theories after they are twenty-five or thirty years of age, so that the ideas which civil servants and politicians and even agitators apply to current events are not likely to be the newest. But, soon or late, it is ideas, not vested interests, which are dangerous for good or evil.```

Note the year: 1936, the eye of the storm that destroyed the western world that once was.


# AI-Uses: Data-Search not Task-Labor?

General Search: Efficiency of search query...
https://www.economist.com/finance-and-economics/2025/10/23/can-ai-make-the-poor-world-richer 

product search: (while people drama-fret over advertising-data)

# Search, Products, and Marketplace:
https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy

Is generative AI a 'market-maker,' and if so, how exactly? Can there positive economic impacts outside of directly creating new goods or "replacing jobs"?

It appears that in 2023-2024 there was a rush, both earnest and bad-actor-hustle, to create products that were re-wrapper re-packaging layers on top of generative models. It is not clear how many people were actually trying to do-tasks with generative or vector based models (arguably that was Rabbit.ai's angle and the public reaction was not great). Broadly speaking it is clear as of 2025 that wrapper-products have not taken-off as a new market (Could that change? Maybe, but there is no clear path for this non-market to become a market). 

In terms of new and old markets and economic impact more broadly, it may be, for example according to this Oct 27th 2025 Economist Article, that there could be a positive impact on market efficiency by improving access to information (a.k.a. a better data search) and improved defense against "bad actors" (their term, not my excessive vigilance). While such an effect may not happen in a simple and clear way, or the opposite might happen, this represents an area that we should be looking at and likely planning around.

Note, this may indicate a significant 'economic transformation' that does not necessarily involve any job loss 'replacing people with robots' or new automation products. Perhaps the general paradigm of the "internet" "search engine" is still evolving in ways we have not been able to predict.

A related if still murky topic is developing world access to an "ai"-type search interface rather than a traditional web-page search (possibly being cheaper to provide and easier to use service) being a 'leap-frog' area (such as where an area without landlines jump straight to a mobile device infrastructure). 

This may also point to "tragedy of the commons" aspects of civics and investment. If areas such as access to information and education are so vital to the economy, why is there so little interest or attention in fixing those problems to make progress? And when something does come along that may greatly improve market efficiency 'lifting all boats,' how is the concept so unexpected and alien? Perhaps we should take perception, blindspots, and decision making skills more seriously. 

Also see: 

democratizing traditional models:
https://www.economist.com/science-and-technology/2025/10/22/ai-models-ace-their-predictions-of-indias-monsoon-rains
...




While this includes insights from Jan 2025

The general mode of mid-2023 to end of 2024, was an inconsistent and self contradictory set of goals mirrored by statements and product releases by most of the main large model producers: google, openAI, anthropic

OpenAI was openly throttling its models (though that might primarily have been to save money). Both google and Anthropic appear to have been deliberately creating safe-diluted models (as in Google's historically inaccurate overtly agenda-driven image generator, to which I have no personal objection). Especially since Mistral focuses on smaller models, it is unlikely that their attempt was to do anything with a GPT5 (which never happened) level super-huge model. And for Meta, which I see as being largely a hero of the story for investing in open source infrastructure: it is not clear if Meta had either the resources or the intent to build a GPT5,6,7...etc Godzilla model. Like Mistral, Meta's focus seemed to be on good-enough small-enough, open source models the people and developers can use. In theory Meta had the money and staff to make a massive model if they wanted to, but, like Apple, making a huge but useless model just for the sake of it does not appear to have been their intent. And Musk's X-AI so far has been a work in progress. I am not aware of any model used by anyone for anything coming out of X-AI, and I have not heard any general discussion of what any product or other goals are. The only thing known is that X-AI may have a policy of open source, though that could suddenly change. 



The overwhelming goal was to not cause social disruption by introducing a mega-powerful AI that would either destroy the universe for example by turning it into paper clips (something people seriously worried about and debated in the uncertainty of 2023) or otherwise cause any panic, confusion, or less than happy feelings around society. Very secondary to this was trying to produce very-vaguely 'better' models according to fictional and incoherent standards, and off the table was serious discussion of actually using STEM to get projects done. A related popular cause-celebre was to simply end all research period (which may illustrate how extremism is common across so called 'right and left' gang groupings).

This stalemate may have been 'effectively' broken by the end of Jan 2025 (though the focus here is understanding what characterized 2024), when one or more models coming out of countries effectively at war with the OECD are claiming to be more efficient and possibly better at some benchmarks (which is extremely difficult to interpret in any practical way). 

The point being that there seems to have been a tipping point in 'overall sentiment' in Jan 2025 about whether people want AI to be better or worse: Where it appears that people want AI to be better and cheaper.

It is still unclear, and maybe forever unclear, if there is any actual thought and intention behind the massive stock sell-off, which very well may have been a panic chain reaction not unlike past high frequency trading feedback loops where random events trigger a chain reaction of selling that no one actually wanted or intended. And the long standing 'tradition' of making up utterly fictional retrospective 'just so' stories to rationalize every up and down of the stock market is a popular recreation info-tainment sport that makes clarity that much more difficult. 

...


Economist:
https://www.economist.com/science-and-technology/2025/04/09/the-tricky-task-of-calculating-ais-energy-use

https://www.economist.com/leaders/2025/04/24/how-to-keep-ai-models-on-the-straight-and-narrow

?https://www.economist.com/by-invitation/2025/04/10/to-keep-on-top-of-ai-focus-on-the-points-where-it-touches-the-outside-world-writes-martin-chavez 


https://www.economist.com/international/2025/04/10/there-is-a-vast-hidden-workforce-behind-ai



Three articles from The Economist about the ~unreliability of generative models:

https://www.economist.com/science-and-technology/2025/04/23/ai-models-can-learn-to-conceal-information-from-their-users 

https://www.economist.com/podcasts/2025/04/30/how-to-tackle-deceptive-artificial-intelligence

https://www.economist.com/leaders/2025/04/24/how-to-keep-ai-models-on-the-straight-and-narrow




In a context of norbert weiner and orwell's warnings about types of dangers, we may need to clarify out interpretation of populist catastrophies of the 1900's as being imposed against people's wills, which if you look at this sentence closely should be clearly self-contradictory: popular trends are not imposed on those people against the popular trends that the people go out of their way to create.

Squid Game, Hunger-Game and Dungeon Crawler Carl (which may still be obscure, while the first two are top-tier fads), all depict a world in which a conspiracy of elites and governments force horrible cruelty on to poor innocent people who are decent and want nothing to do with horrible cruelty. But this 'bait and switch' is a grotesque and dishonest way to have your cake and eat it too. People are popularly attracted to and enjoy horrible cruelty and torture, there is simply too much historical evidence to try to whitewash the record of homo-sapiens humans, yet in perhaps a form of silver lining people may not be entirely comfortable with their insatiable thirst for a shadenfruda-sugar-high of hyperbolic pain and suffering: biological homosapiens humans seem to insist on getting their hate-and-torture-thrill-fix wrapped in a package of innocent selfrightiousness: it's 'them' that are doing this to 'us' and...that's it, we are justifide in wanting 'them' to suffer because cliche external-local of control meets infantile refusal to take responsibility.

While this may be, or seem to be, a tangent not related to AI, it may be important to pay attention to these trends as some companies seek to use 'short term thrill' feedback loops to train AI to give homo sapiens humans more of what they want. 

If what people want is the thrill of causing suffering as is depicted in the oh so popular and attractive dystopias of Squid Game, Hunger-Game and Dungeon Crawler Carl etc., then that is what homo-sapiens humans will be training AI to become better and better at creating: dystopia, deception and lies, with a focus on ever more extreme suffering and cruelty.

If we do not want AI to become increasingly good at creating more and more intense dystopia, deception and lies based on ever more horrific pain and suffering, then we may need to take steps to learn from history so that we do not repeat it, and part of that may need to be ending the 1970's delusions that everyone is perfectly rational (which was a punt never defined) and that feeding every whim and short term impulse is the only way to become enlightened and responsible: learning of all kinds is not automatic, instantaneous, etc. see def-behavior studies.

..

2025 05 19 world in brief blurb: economist

The toxic side of AI chatbots
Close-up of holding a smartphone in front of an LED display.
GETTY IMAGES
AI chatbots sometimes blurt out inappropriate responses to users’ queries. Grok, a chatbot developed by xAI, Elon Musk’s AI firm, recently ranted about a supposed genocide against white South Africans in answer to questions about the weather, professional wrestling and cars. The company blamed an “unauthorised modification” of the software that violated the firm’s “internal policies and core values”.

Yet chatbots often give controversial answers to innocuous questions. Researchers from Microsoft recently studied how often large language models generate “harmful content”. This includes political and scientific disinformation as well as the promotion of violence or racial hatred, self-harm, criminal activity and sexual exploitation. They found that almost 27% of responses were harmful, regardless of users’ input. That is because of the data on which LLMs are trained: the researchers found widespread toxic content in the vast datasets used to develop popular AI models. “Guardrails” designed to keep out such material, such as keyword-based moderation, have their limits.


...

# Interesting Confusion around Sutton's 'Bitter Lesson' article

the bitter lesson
Rick Sutton

- https://transformer-circuits.pub/2025/attribution-graphs/biology.html
- https://github.com/Jeffser/Alpaca/
- https://www.economist.com/business/2025/09/25/how-ai-is-changing-the-office 

The 'history of the future' and investment patterns around the metaverse:

tech stack instability.


in theory 
generative/embedding AI, subsymbolic stem, project-state technology, virtual-reality/meta-verse, internet virtual communities, will come together in the future. Why have they not come together to 'transform the world' already?
1. We should not be looking for a divine-shock transformation
2. The state of the science and status of technology infrastructure is nascent/juvenile and in shameful shambles.
- power of 10 rules retrospective

https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf 



# The Question of Continued Training and confusion around Sutton's so-named 'Bitter Lesson'

Richard Sutton – Father of RL thinks LLMs are a dead end
https://www.youtube.com/watch?v=21EYKqUsPfg 
- experiencial-learning...
- 'pretrained' vs. continuous training...
- learning and re-training...
- ...project spaces...
- 

...
2019 'the bitter lesson' sutton article...
...

[END]


////////////////////////////////////////////////
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
////////////////////////////////////////////////
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
Triage Area:

..
EDIT HERE

The discussion is all over the place. No one I know, and I have worked in AI officially since 2020, is even interested in AI. Even Data scientists are not interested. People starting AI companies have no interest in talking about AI. The pundits on the news say that 'everyone' is talking about AI, but I am not aware of a single actual discussion "column' in any news source. 

People are hype farming, and lamenting hype, but there is amazingly little actual discussion. 

No one has any plans for what they want to do with AI other than art-project.
Art is not bad

...

I am skeptical of many people who want to use AI for 'coding.'
A massive problem in computer science is that people spend years and years of their lives making art-project 'coding environments' but not using them to do any of the 50-year backlog of nightmare needs that the software world has built up. 

Again: AI Fantasy appears to be what is on the menu. People want AI to entertain them. People who had zero project plans before AI, and who were primarily interested in junkfood entertainment and 'self care' before AI, are not suddenly going to be different people with different goals now that a few terrible AI tools are available. A badly planned restaurant startup (to pick a random example not connected to my own work history) where people lack a coherent plan, real world experience, and the ability to tell that start of a schedule happens before the end (which in my work experience most people amazingly get wrong constantly),  is not going to butterfly-transform into a dream-fantasy because of garbage chat-AI, or even because of a decent-chat-AI that didn't exist by the end of 2024. "I'm bored. Entertain me." + AI is not a black box of highly skilled productivity, literacy, ability building, life long learning, project management and decision making, sustainable productivity, sound investment etc. 

We should not be surprised if 'junkfood and fantasy' are and continue to be themes with AI. 


...

ai in gaming...

...

voice acting work:
- 
- 

...


...



# Links




https://www.economist.com/briefing/2025/07/24/ai-labs-all-or-nothing-race-leaves-no-time-to-fuss-about-safety

https://www.economist.com/international/2025/07/29/how-spy-agencies-are-experimenting-with-the-newest-ai-models

https://www.economist.com/science-and-technology/2025/07/23/fragmentary-latin-inscriptions-can-be-completed-with-ai 

https://www.economist.com/interactive/2025/07/23/inside-the-top-secret-labs-that-build-americas-nuclear-weapons



https://www.economist.com/business/2025/07/28/how-big-tech-plans-to-feed-ais-voracious-appetite-for-power

https://www.economist.com/business/2025/07/31/who-will-pay-for-the-trillion-dollar-ai-boom 


messy unclear article:
https://www.economist.com/international/2025/07/31/could-ai-tilt-the-outcome-of-elections 

https://www.economist.com/asia/2025/07/29/south-east-asia-makes-an-ai-power-grab 

https://www.economist.com/leaders/2025/07/31/america-is-easing-chip-export-controls-at-exactly-the-wrong-time

https://www.economist.com/science-and-technology/2025/07/30/china-has-top-flight-ai-models-but-it-is-struggling-to-run-them 


https://www.economist.com/science-and-technology/2025/07/31/scientists-want-to-sequence-all-animals-fungi-and-plants-on-earth 


https://www.cnn.com/2022/02/22/politics/mitt-romney-russia-ukraine 


...

James Gleik Article in NY review of books


...

data scale:

wikipedia
7.4k books

barns & noble
100k books

jstore:
12 million articles

amazon com
32 million books

google has digitized
40 million books
...


...

A note on the founders (of AI):
As I very inexpertly attempt to explore in the paper 'AI broad or AI narrow' the story of the founders of AI themselves is itself significantly obscure.

The ambiguity around "AI" around 2024 is less a new low cloud fogging a landscape, than a typical climate of low visibility and numerous obstacles.

In some ways, the story of AI can be strikingly clear, for example looking at Johannes Kepler's method as being essentially the same as Machine Learning, unifying STEM overall as a kind of bigger picture 'scientific method.'

Language, concepts, meaning, and problem-solving are domains that are so murky that we do not know if they exist as such. Process, projects, values, and best practice, and maintainability, though the discussion arguably has roots in the same pre-science literature such as Thomas Hobbes's leviathan, are areas that biological humans appear to be hell-bent on suppressing. People are famously not fond of dentists, but dentistry begrudgingly exists. When it comes to a choice between data and potemkin villages, the determination is more one-sided. There is no contest: fantasy rules all the way to the grave. 

It is something precious that Jeffrey Hinton did not win the same award as Giordano Bruno; we must strive to keep learning.



...


Michael Wooldridge | AI History
https://www.youtube.com/watch?v=Zf-T3XdD9Z8 

56-74 as golden age of GOFAI
combinatorial explosion
np-complete
search & AI
sub-symbolic search
vector-search...?

1. SHRDLU purely abstract micro-worlds
2. rule based MICIN expert systems
3. 


...


...




...

- broad-band

tech difficulties and tech-stack brittleness (bad practices and demand-distortion (hordes of idiots destroying markets by demanding the wrong products). 

https://www.economist.com/science-and-technology/2025/09/22/why-ai-systems-may-never-be-secure-and-what-to-do-about-it 


note:


Paths to Models: General Options

1. make / train / host
X
3. in-house-local, in-house-cloud, luxury-service


... 
Voyager is still going strong. Not many things made since then are though. 


...

# pretraining and maintainability
What is the cost of retraining a pretrain-is-final model, what are the cost projections, are people costing this into their plans?

The glory of "Moore's Law" is the downward trajectory of overhead costs making more possible. 

Unsustainable Ballooning of resource requirements should be a red flag. 


...

language concepts vs. skills-learning

ALU vs. guestimation


...

Coding-ai still has extremely serious problems and marginal usefulness

...

To what extent is the pop-fad of 'gen-ai' lost in a social-stimulation feedback cycle of a classic gang-thub-sport addiction of beating erroneous benchmarks, and either celebrating with riots if you 'win' or protesting with riots if you lose? 



...
Same Day Paradox: sept 2025.09.09

"Figure of the day: 95%, the share of organisations that are getting “zero return” from investments in generative AI, according to researchers at the Massachusetts Institute of Technology. Read the full story."
linking to this:
https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up


and headline story is:
sept 2025
https://www.economist.com/business/2025/09/08/faith-in-god-like-large-language-models-is-waning
the full title is:
```
Faith in God-like large language models is waning
That may be good news for AI laggards like Apple
```

"An American antitrust ruling this month allowed Google to continue payments to Apple to position its search engine prominently on the iPhone; the two may be working on a plan to partner on ai-related search. That ruling started a revival in Apple’s share price, which has lagged its big-tech peers this year."
...


# a Look at statefullness

- longer context windows: the problem of short context windows and the illusion problem of long context windows.

- no out-of-text instructions

- single-reflection smart mirrors (with zero memory or state)

- no (or too little) dynamic use of vectors (manifold hypothesis vs. stochastic generation, vs. static vectors)


...

# CP Snow's two culture and biz-bro jocks:

To what extent has a consortium of half-educated engineers (lacking any cultural education) and business-sport-masculinists (lacking either STEM or humanities education) have simply not noticed that their 'gen-ai' tools are pitifully stupid?


...

# Joseph Cambell on Cameras and AI-investment as recreational fascination
economic models of computer engineering and popular pursuits

How much of the investment in AI is because people enjoy it, and can we consider this a positive thing?

For all we can grumble about it, popular 'gen-ai' may be a watershed period where more people can interact with STEM tools. For example, The Economist mentioned that for open-AI one of the fastest growing markets and countries/regions where there is most wide-spread adoption is India, and that the interface needs to be modified because many of the users are illiterate. 

To 1997-style techno-utopians, this should be a very interesting kernel. There is now a popular computer interface that people can start using before literacy. Sure ...there are probably a few risks we could think of to go along with this (perhaps John Adams saying that an illiterate population cannot run municipal administrations directly for tautological reasons, and his unpopular view at the time that much of the French Revolution was extremist-populism, not skilled engineering)... but there is bound to be an upside here: access to ways to gain literacy, access to education and medicine, etc. 

We should probably look at the history of the computer industry to see who invested and how and why, to do what, and what was sustainable and maintainable.

The 'maker' and 'hacker' communities have long been photoshopped out of the picture, but altering the data is generally the wrong policy.

Many systems that took lots of investment directly, or indirectly (and that may be an important distinction), collapsed. Visical was going strong and selling computers...until it died. Lotus 123 was ... until it died. DOS was ... and died.

And locked-in software for education and defense are often abhorrent nepotism rat fight charades around nonsense-code potemkin village software, not serious STEM and ~Agile.



...

A

Read the old books.

Reading list for 2024:
leviathan, hobbes
berlin diary, shirer
The end of history, Fukuyama
zbig, ?
strategig vision, Brezinski
strangers and intimates ?
first folio, shakespeare



...
names and views:
- wolfram
- gliek
- chollet
- hinton
- Wooldridge
- Hofstedter
- Mitchell
- Pinker
- LeCun

...

...

possible clear meanings of 'ai' from other paper...
- STEM
- any previous done only by human task task-doing tech
- specific tech such as regression or decision tree or ANN
- very specific tech or with commercial brand of that tech
vs.
meaningless arm waving

...

Zbig 'strategic vision' and the Russia/USSR marxist-pseudoscience vs. western 'lack of "order"' 
may be a more approachable context for symbolic vs. subsybolic AI.

...

from np-completeness bottle-neck in symbolic problem space

to energy-costs and hardware

question: Racks - 

...


supposedly 'marx-science' was made of orderly attractive aesthetically pleasing belief-friendly symbolic concepts and modules, and over decades we repeatedly saw two things:
1. that there was no performant connection between these untestable undefinable beliefs and reality: people starved to death instead of flourishing
2. people were utterly addicted to the irresistible mind-candy super-signal marketing alure of what sounded so 'ear-kissing' (to use Shakespeare's phrase), such that even after decade after decade of extreme and grotesque refutation people simply refused to abandon their delusions and continued to shun reality regardless of the result being their own destruction.


AI was similar until 2012 when Hinton's team showed the world something that somehow broke through people's ability to ignore reality and be in denial: image-net.

'symbolic' AI (of various kinds from GOFAI and expert-system to hand-directed statistical learning (regression, decision trees, logistic probability, etc.) had niche use-cases but was not a universal solution to everything.

confoundingly named "sub-symbolic" AI was raged against, and regardless of it not being how people want reality to be structured, it works.


reified fantasy-abstractions vs. concrete process

there is something addictively attractive as the history of 'religion' may attest about reified fantasy-abstractions
- external locus of control
- ? sports psychology
- the never-ending nonsense explanations for hourly stock market movements
- D.kanneman on hallucinations out of the random outcomes of a small number of business events that spur impassioned mythologies of belief about cult of personality magic and the cargo-cults that follow


space for play, and the dangers of 'the arena'



values + process + empiricism + STEM vs. ideology, fantasy, and potempkin villages


While STEM iterates in the long term, it has been noted that science progresses funeral by funeral. Even people who believe or claim to be operating scientifically can be severely divorced from either reality or any kind of STEM feedback. 



...

notable somewhat remarkable 
contrast and lack of contrast between
2022 fransoi chollet and 2025 fransoi chollet
and compare this with Hinton's retirement talk during 2025

...

Interpretability: Understanding how AI models think
https://www.youtube.com/watch?v=fGKNUvivvnc 



the effect of windows 8.1 on my aging father 


Analogies for pseudo-state and illusion of state:
'ex-machina' games in games in games, plays withins plays within plays

- can be practical
- not a sensible long term core module
- people (biological h.sapies-humans) are either self-diluded, incapable of understanding, or haven't been educated, about the 'fake-ness' of the entertainment-skin of psueudo-state
- this is not a fundimental limitation of technology: a model can be trained on memories
-- a video/audio/transcript claiming to be about you but that you do not have any memory of (and which can be altered, and which you are warned is likely to have been altered)
- 
A. learning not from a 'self' point of view
B. memories of experiences in general

taking the internet, grinding it up in a blender into pieces, and force-feeding it through a neural-net learning system: no 'self'

-- where is developmental behavior and psychology?
-- where is 'education-ology'?
-- where is 'mind-ology'?
-- where is general STEM?
-- where is value-function-meaning (ethics)?
-- where is sustainability maintainability vs. collapse?
-- where are non-automatically-learned skills?


gamification?
..


- summer, winter, boom and bust
- power supply
https://www.economist.com/business/2025/08/28/how-a-power-shortage-could-short-circuit-nvidias-rise 
- chip supply
- cuda-configuration
- performance gaps
- lack of basic project/product management
- overreach and unstable ecosystem-extinction
- ai-fake-STEM vs. ai-fantasy-sport
- 
- no meaningful discussion of topics such as Object Relationship Spaces or trying to zoom in on what is happening and what is not working

...





context-window use and expansion:
-- crutial area but not often discussed
the economist often writes about GPU/TPU topics,
I do not recall the Economist regularly discussing context window dynamics



...
Known, Unknown, and Debated, Undebated

A. writing/publishing up until 2023
	- non-deep learning, or non-very-deep learning
B. What changed in 2022/2023?
C. Biology and Deep Learning: Debated
D. Subsymbolic-STEM & General STEM: not even debated yet

stateful questions
externaliztaion
GPU/matrix-compute in hardware

...
clearweb deepweb, new hybrid?

...

long term data storage
...

tpu -> DNA

...

https://www.economist.com/science-and-technology/2025/07/23/fragmentary-latin-inscriptions-can-be-completed-with-ai 

where actual use-cases are rare, and their rarity seems to be covered up for whatever reasons,
this case may give clues to both the current provisional (not great) results, the gap between the fantasy of 'total black box project autonomy' and the ongoing much lower bar of collaboration (and perhaps the persistent erroneous insistence of AI as an absolutely separate other worldly entity (maybe strangely parallel to judeo-cristial extreme-abstraction of even everyday values and ethics)
and also perhaps a foot in the door to the strange absence of crowd-sourcing and non-professional projects. 

..

The Opaque Frontier of Computation: TPU, APU, GPU, NPU, CPU, etc. etc.

https://www.economist.com/interactive/2025/07/23/inside-the-top-secret-labs-that-build-americas-nuclear-weapons




...

From mysterious Nvidia selloff to September forcaste-beating. 


...


...

Hinton:
https://www.youtube.com/watch?v=IkdziSLYzHw 

maybe similar but longer format:
The 2025 Martin Lecture featuring Geoffrey Hinton — Boltzmann Machines
https://www.youtube.com/watch?v=juif0T8NOsY 

subsymbolic STEM

the manifold hypothesis

language linguistics and learning about learning-language


https://www.economist.com/finance-and-economics/2025/07/17/why-is-ai-so-slow-to-spread-economics-can-explain 

https://www.economist.com/business/2025/07/14/ai-is-killing-the-web-can-anything-save-it 

Paradigms of Mind:
- religious cults, gangs, "sport" without "sportsmanship"
- internet misbehavior (the inexorable tendency)
- 1970's naive psychology positivism
- 1870's naive broad linear positivism
- mind and mind-cultivation
- attractors and equilibria
- K&T
- mind expansion 

The Market-Pundit's New Fudge: "Valuations rose/fell because of AI."
- 
- Apple


..

context window size
&
querying structured data


stateful questions
structured-query questions
scaled questions


...


# AI and search, and the unsolved problems of search

while, if strangely, perplexity is not as of 2025.07 especially promenant, so far the only area that seems to actually be significantly impact by AI (outside of hype investing and doom-click bait reporting) is that people prefer the interactive chat interface to a traditional search engine for querying the web.

https://www.economist.com/business/2025/07/14/ai-is-killing-the-web-can-anything-save-it 

there are a number of questions and issues around this:
- is this the only thing "Gen-AI" will actually shape (though it may be a big thing)?
- will the old design problem of 'no micro-transactions' online be addressed?
- How many people are really worried that the dumpster fire that the internet has become might be changed by chat-search? 
- what kinds of queries do or don't work well?
- is that article title supposed to be a joke, as the internet becomes a meaningless pile of 'kill' meme nonsense. Will AI-search change the demand-distortion nightmare of populist demand for fantasy 'kill-drama' to cover-up a repellantly nuanced non-vapid reality? 
- revenue: the court is still out on whether mega-models can be profitable

Note: the case of using mixtral 8x7 as a search engine during a power/internet outage to research the 1991 AT&T vs. UC Berkeley court battle over Unix.


# The Importance of Data


Economists don't know
https://www.economist.com/finance-and-economics/2025/04/24/economists-dont-know-whats-going-on 
....

(Read William L. Shirer's 'The Collapse of the Third Republic')

...
local vs. cloud costs?

...


Document Processing:

- metadata & vectorization

- pdf forever nightmare

- rag vs. 'context window'

- generative guesstimation vs. vector analysis
...


# Moderation, Modernity, and AI in an Educated World
- Beatles Mania
- Third Rieche Rallies
- 

...


note: fransoi chollet in 3rd: ed:
20k in cost to answer one test-question...

...

AI costs...
https://www.youtube.com/watch?v=mRWLQGMGY80

..

people are starting to talking about the ceilings and bottlenecks
https://www.youtube.com/watch?v=nyvmYnz6EAg


..
significant confusion around the use of the term AI:
what are people referring to?


Double-Garbage: People using completely undefined 'ai' to do a completely undefined project: not a recipe for process-based pragmatic and productive success.

..
...




...

..
change china section may be add the china mission...


change 'no' to 'few' e.g. few people discuss AI


STEM and history as pejorative:
have we found what works?
note:
'ai' as the same method described by... Kepler?

...


divergence of models and benchmarking techniques

...

overtraining and fragility of model

...

# people love the sport-theater-drama of cargo-cult pageants and fireworks:
- khaneman tversky and theatrical fictions around stochastic fluxuations.

..

Alignment and the Genie in the Lamp

In 2023 when AI was rapidly becoming less completely useless, there was a mass popular frenzy to 'align' AI to think and act just like people do, and a mass panic around AI being any way foreign to how people think, act, behave and make decisions. The presumption, sometimes stated outright, is that only by making AI identical to biological h.sapiens humans, only that is the magic silver bullet to ensure the AI behaves with perfect ethics, transparency, and foresight. Again, a classic repeating story in data science, machine learning and AI is the genie in the lamp problem: we set out to achieve a set of goals, and it turns out that what we wished for is somewhere between not what we really meant and not what we should have wished for. It has been and remains a minority of humanity that dares to be burned alive by the majority of humanity for standing up for what is right.

..

? Abstraction...


...

Does this claim that llm-assisted coding is less efficient?
https://dora.dev/research/2025/



...

Business model of software:
- visicalc
- lotus123

...




Economics and Buisness Models:

The software that sells computers
- visicalc
- cuda (indirect)

Post-Mail
Email
Servers
Search


...

# AI As Search: Vector-Search / Matrix-Search




...

# AI Brainstorming vs. AI-Reasoning: Matrix Navigation and Search

Possible Aspect of 'reasoning':
I think I have observed that even if 'reasoning' helps the output, the 'reasoning' is 
A. clearly make-believe
B. often word salad

It is possible, naming things is hard, that 'brainstorming' might be a better description of what is sales-pitch-named 'reasoning', as the model is definitely not creating a stateful plan and then deterministically executing it (though that could be part of a multi-step architecture...I build one in 2023, but models were still too chaotic to follow instructions). Brainstorming even for people as a part of planning is usually overlooked, not valued, not invested in, and a point of skill deficit. 

This might seem like an overly nuanced 'possible' or 'semantic' distinction, but the point is that if you see incorrect garbage in the (so called) "reasoning" that may not mean that what you see is an input or assumption of the AI in any way: it may be expected that the "reasoning" is even deliberately noisy (full of false statements) in a way to better tune the final (manifold) orientation.

More philosophically, this may call into question the (common in 2025) approach of "we can see what the AI is doing and how it is approaching a problem by looking at the reasoning, it says 'xzy,' so..." Clearly this is not literally true if the "brainstorming" contains a large amount of incoherent "scribble" noise, which may nevertheless be an important part of better "searching the matrix space" for a more accurate final action. 

There needs to be responsibility in journalism and peer review at least, even if 'marketing' is sometimes given a pass. 

...

A terrible STEM-illiterate fear mongering articles that gropes at fear-buttons while muddying the important details of the topic. 
https://www.economist.com/culture/2025/10/30/strongmen-in-politics-and-technology-are-changing-the-world

...



# Appendices

1. CEO of Anthropic

2. CEO of HP


(TODO, for the series...)


///////////////////
...

1996 vs 2023

...

https://www.economist.com/science-and-technology/2025/11/11/the-promise-and-the-perils-of-using-ai-for-therapy 

- does not mention ELIZA... ?


The story of ELIZA is so nuanced and convoluted that I am not going to try to describe it authoritatively. Joseph Weizenbaum (at MIT) created/finished ELIZA in 1966, making it one of the first full programs of any kind (again, the timeline is not intuitive; chatbots lead to programming languages, not the other way around; ELIZA was so early Weizenbaum had to write is own programming system and language with which to write ELIZA (B in 1969 and C in 1972, both by the inestimable Ken Thompson & Dennis Ritchie). No one agrees on what happened next. Some say Weizenbaum created ELIZA either purely academically to study human-machine interactions (without any assumptions), others say that Weizenbaum created ELIZA deliberately to show that people would see the superficiality of a minimal bot that only had canned responses. 

There are also multiple sides to what happened later
1. Joseph Weizenbaum became extremely distressed when it became clear that many (normal, adult) people projected full human-ness onto the minimal chat-bot. Weizenbaum wrote a now out of print book (https://www.amazon.com/Computer-Power-Human-Reason-Calculation/dp/0716704633) arguing that machine-intelligence was impossible and largely dropped out of the limelight, though he did play a key part in supporting Kathy Keimans' research rediscovering and documenting the early days of computer programming including the ENIAC Six see: "Proving Ground" https://www.amazon.com/Proving-Ground-Untold-Programmed-Computer/dp/1538718294/ 

2. A different parallel track is that various people and researchers, including Xerox Park's Prof 'Skip' Ellis (my professor, employer, mentor), saw practical uses for ELIZA type systems, noting that for some Patients the (however deliberately primitive) ELIZA bot was a more effective (infinitely patient) therapist than real human therapists for some patients and their positive outcomes. 

https://en.wikipedia.org/wiki/ELIZA

I invite you to explore the many-dimensional ongoing rabbit-hole that is the mysterious story of ELIZA, Weisenbalm, and chatbots, et al. to decide for yourself what the story is.

https://en.wikipedia.org/wiki/SLIP_(programming_language) 
https://en.wikipedia.org/wiki/MAD_(programming_language)


...



...

https://www.economist.com/the-world-ahead/2025/11/10/ais-true-impact-will-become-apparent-in-the-coming-year 
"...hype and the hopes around AI have been like nothing the world has seen before, .... Will it bring an economic revival, a financial bust or a social backlash, or some combination of the three? The world will start to find out in 2026. "

A. There is no explanation in the short article how 2026 will be any different from 2023, 2024, or 2025. Likewise there is no discussion or insight into how it is so unclear whether a 2017-2022 technology has heretofore undiscovered yet totally suddenly world changing applications. After four years?? 
The reminder that "According to America’s Census Bureau just over 10% of businesses with more than 250 employees say they have embedded AI into their production processes. A survey by the Massachusetts Institute of Technology released in July found that 95% of businesses’ AI pilots failed to generate a return at all." With the caveat that "production processes" is not clear, the overwhelming circumstantial evidence is that there is no, there are zero, low hanging fruit general adoptions, applications, use-cases for generative-AI chatbots; there is no wrapper-market; there is no integration market. Chatbots (stateless, amnesiac, non-learning, random-guess generators) themselves may be a new class of product with overall effects on markets (such as the 'ripoff-economy' article saying that chat-bots can improve information-flow about goods and services and reduce market inefficiencies: that is good news indeed, if less emotionally addictively attractive than doom and gloom arm waving).  


B. The article's non-elaboration, "A key indicator to watch in 2026 will therefore be the rate of formal adoption, and the extent to which these efforts are succeeding." 
is a rephrasing of the headline, not an explanation or evidence. How much would you bet that at the end of 2026 there won't be a rinse-and-repeat headline about how 'We'll know for sure in 2027!'

C. The idea that claims about technology are "unlike anything before!" is amnesiac fraud, be it naive stupidity or bad-actor disinformation. Technology and modernity related mob-fuel has historically resulted in ruinous consequences, such as the 1800's 1900's socialist-marxist-communist narrative that magic technologies would, literally, allow birds to be cooked in the air as they flew into worker's mouths, hype that drove movements to push populist authoritarian illiberal regime-change that then killed tens of millions of those same poor workers and others. Journalists, a core check and balance institution in civil-society, have a fundamental responsibility to not be profoundly, self harmingly, ignorant and unintelligent. Repeating vapid, lethal, false, headlines is inexcusable malpractice.


The term 'agentic' has gone where meaningless terms will eventually go: nowhere.

- Externalization
- Architectures
- Project-State




...

"search" and AI
..
lack of meaningful tests
..

State and an Illusion of State

..

platform and API is unclear
likely pointing to a general state of disarray with software in general.

Perhaps because GUI environment are so diverse and cumbersome,
headless command-line (cli) systems are still the gold standard,
yet that creates a gulf between the realities of what developers and develop and what users can, will, and are able to use.

..

Education may be a kind of special case, where everyone should welcome as much AI help as can be offered. Schools are notoriously, tautologically, difficult to fund and support as they are a classic tragedy of the commons with economic gains being realized far away and long in the future. 

Anything that can be sustainably automated in any way may be drastically needed to make education sustainably affordable and maintainable. 

..
HR needs

the state of hiring and admisions
..

mind
..
history
..
stem
..
learning-ology
..
project management: neem
..
production data science
..
Agile and AI

..
KT and AI (khame tvstsky)

..
1791 and 1971 (the french revolution and nixon closing the gold window)

..
AI and the psyhology of modernity
AI and Education
AI and general STEM

..
Mind, Development and Behavior:
- bad equilibria:
Sport Entertainment as a suicidal homicidal death cult

..

Data 1: RUnning out of Data

Data 2: "Synthetic Data"

..

Model Fragility and New Issues since 2022
https://twimlai.com/podcast/twimlai/is-it-time-to-rethink-llm-pre-training/ 


...

# Aspects of Data and Data-Sourcing

background on attention paper:
https://twimlai.com/podcast/twimlai/the-decentralized-future-of-private-ai/ 
Illia Polosukhin, a co-author of the seminal "Attention Is All You Need"

- user-ownership
- open source
- long term risks of bad management
- 

...
docker-hardening?



/////////////////////////////////
/////////////////////////////////////////////////

removing adjectives?


# Misc:
- The hiring market is in total choas; no one has any idea what's going on while everyone feels the need to pretend they do. 

..

Technology for Education:

The strange mismatch of Sal Khan talking about AI on bloomberg, the stark self contradiction of 'moving so fast it makes your head spin' and 20 years of roughly zero progress.


..

Learned more about AI/ML more broadly.

2025.01.12

..

An information immune-system that defends against disinformation attacks.
Supporting education.

..

...

# Shrines vs. Content

- 'writing desks'
- developement environments
- 

...

General Learning in the news:

Non-Automatic Learning:
There is a not uncommon view that history will not repeat itself because people will automatically learn in real time.

..

Was 'fat tails' after the Leman-Shock the only concession to Mandelbrot and more then a century of wrestling linear and dynamical models?




..

What is the current state of embedding models?

word embeddings...
sentence embeddings...
document embeddings...
(does that distinction make sense anymore?)



sherlock holmes
sigmond fruid
detective stoires

Edmond Berk?

Shaw Chesterton & Project management

1939-1940: Pigmalion in Berlin



McKenna: wwii was one of the mos science-fiction times in history.
finneganas wake


we are awkwardly rising from the ashes of worlds destroy in that 18 and 1900's, but faltering and repeating remedial mistakes.

..



...


The Moneyball "Controversy"

... 


surrounding world in 2024
- OS
- DB
- network infrastructure

bugs, security issues, constant malfunctions
massive bloat

..
FM-radio and technology: hypoxic deadzones


still in the middle of paradigm shifts
see hinton fefeli video



...

# Maintainable Development


FM-radio and technology: hypoxic deadzones

...


...

The eternally bizarre fake-rage over less irrelevant and less annoying advertising.

https://techcrunch.com/2025/10/01/meta-plans-to-sell-targeted-ads-based-on-data-in-your-ai-chats/
...

HR Tools: 
There is a need for qualified people to be in the right roles.

sinapore a model?


Continual Learning
Feedback

learning and project management areas?



let's review the life-story of Hilbert


...


Pending: 2024 came and went and the 2nd Edition of 'Natural Language Processing' is still in production: personally I'm glad and think in retrospect a re-written edition will serve the world better. 


...

Recommended reading:

The Fall of the Third Republic
AI Narratives
Noise: Danniel Khaneman
Issacson: the innovators
Kahneman: Thinking fast and slow
Gleik: the information

Books:
- AI Narratives is an excellent background study of the Modurnity-Panic the fueled the destructiveness of the 1900's. 

- First Principles is a, however flawed, excellent look at how history was viewed in history.

- SPQR was, I am not an expert on Roman history, a wonderful survey of Roman history so far as I can tell and a great foundation for then reading 'First Principles'

- Cahoon's from Hobbes to Habermas is, as an overview, a highly recommended overview.

- William L. Shirer's 'Collapse of the Third Republic' is likely an important book to read the OECD world is experiencing the same extremist-pressures as arose as populist movements (Fascist, Communist and however pigeonholed) in the 1900's, though not knowing which countries will, if by a roll (or role) of the dice, side with extremism or moderation, it is too early to say exactly how Shirer's writing are important for whom. Indeed, Berlin Diary, Rise and Fall, and Collapse of the Third Republic should be books that everyone on earth reads and rereads in 2025, though inevitably this will not happen and profound ignorance will be a shaping force for the worse (whether marginally or catastrophically).



...


https://www.economist.com/business/2025/12/01/lessons-from-the-frontiers-of-ai-adoption 

maybe a tacet admission that no new technologies are coming, and it really is about 'using chatbots' to 'autonomously do tasks' which is, on the surface, a mismatch.

this is also an example of an unhelpful click-bait type article where even though the article repeatedly says that there is no evidence of either job losses to AI or because of AI or 
B. that any technologies that can replace jobs are even known to exist
that 'arm wave' 'smoke and mirrors' 'gloomy rhetoric' 'doom' the using every trick to fear-entertainment say the opposite of what was factually stated.

...

https://www.economist.com/finance-and-economics/2025/12/04/ai-misinformation-may-have-paradoxical-consequences


...

https://www.youtube.com/watch?v=I9aGC6Ui3eE 

https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html

https://www.sas.upenn.edu/~cavitch/pdf-library/Nagel_Bat.pdf

What is it like to be a thoughtless inanimate object?

We seem to be, as usual, as to be expected because it is usual, to be vacillating between absurd absolutes.

1. The 'old' fad: 'intelligence' is exceedingly rare, it only really exists in a transdimensional alien male deity, and is shadowed and echoed only in a few violent 'great' men. Everyone else, everything else, is evil, earthly, soul-less, and effectively inanimate. All animals are in all ways inferior to people (aside from the fact that h.sapiens are animals), and all people are in all ways inferior to 'great' men (who are more divine than human). 

2. The 'new' fad: everything is intelligent, everything is intelligence, and everything is absolutely equal. My socks are in every way as intelligent and mindful as Euclid, Aristotle, Cicero, or Newton. Most animals are more intelligent than most people (aside from the fact that h.sapiens are animals).

How a person would disclose or fail to disclose their orientation to this relevant to context spectrum is significant for how we should interpret their statements about how a search-engine feels about ethics. Is this a nuanced empirical observation about concepts in the search-space (which is a map of concepts and where ethics are often relevant in searches)? Or is this some kind of extreme ideology that either turns into an external locus of control conspiracy theory about deities and all powerful supernatural governments(for example controlling the weather with fiat dictated commands), or about how the search engines might swap observations about the nature of feelings with a pair of socks and statefully manage a mercantilist economy consulting with a dolphin.

In between the extremes, there are many people who understand these distinctions at least on a basic level, many people who do not, and no observations so far (so far) of 'non-people' who do.

And there is of course the standard 'person-definition' problem: h.sapiens define the qualities of a participating 'person' as being ...' h.sapiens. This is the ridiculous tribe-gang refrain of: might makes right, we are the only people, you are either one of us, a slave to us, or destroyed by us. Pathetic, as STEM models go.

The main predictable 'edge cases' to keep an eye on are:

1. extra-terrestrial 'alien' persons (no evidence that they exist and are now here on earth, but even less evidence that they can't or don't exist at all).

2. Genetically Engineered 'animals' and hybrid species. 

3. Genetically divergent and engineered species branching off from classic h.sapiens.

4. "AI"

5. Biology, computer, nano-tech hybrid chimeras (for example, such as are not clearly in any of the above categories, though there is a large spectrum here: a person who is wearing glasses or a T-shirt is usually still recognized as being human).

According to the 'DNA-test' for person-hood, none of these are 'people' yet all would be meritocratically equal to "classic-dna-h.sapiens-humans' in societal participation and interaction.

https://www.economist.com/business/2025/12/14/job-apocalypse-humbug-ai-is-creating-brand-new-occupations 

...

Fanticists love Abstractions, Slobs love slop; It's pretty clear where this can lead: https://www.economist.com/interactive/science-and-technology/2025/12/10/the-next-version-of-the-web-will-be-built-for-machines-not-humans

...

awk

subtle or nuanced 'learnings':
- how pervasive it is for people to indefinately inhabit social-bubbles of perspective and believe in a deleterious way
- the pernicious eroding force of negative harassment (trolling, nihilism, etc)
- 

...

Due to the lack of meaningful tests.

? Sam Altmans social-affirmation blog post:



...

No GPT5, and no original GPT4 even...

Gemini and X-Grok were useless.

Where are: 
- IBM
- RAND
- NIST

Where is Academia?



...

Arxiv:
- increase or continuation in articles 
- status quo: articles not broadly read or discussed


..

The Not-Reading Problem

Isaccson's innovators... or know who Khanaman & Tversky are.
They haven't even read "Moneyball." 

see: recommended reading list



..
...
over-reach

# Profitable AI and economic models for profitable software

the unclear history of software 'economic models' or product profit plans:
- steve gibson: when software becomes 'finished'
- subscription software
- 
- 


Not Easy to Make Useful:
https://www.economist.com/business/2025/05/21/welcome-to-the-ai-trough-of-disillusionment 


slowly articles that are less terrible in the economist


developmental psychology and childhood development

general mind studies


Two high level mentions of main players not knowing how to use test-results in model training (2025.05.03)
1. Zuckerberg saying the tests did not exist and the path unclear
2. a report that open AI reverted from a whole product-deveopment set of performance updates because the results were so bad.


Table of contents

Topics:
1. The genie and the lamp (problem/puzzle)
2. 

'alignment' meaning project alignment vs. 'alignment' meaning inexplicable-agenda vs. 'alignment' being a duplicitous coverup of the fact th at humans are duplicitous and that AI is trained on bad-actory bad-faith toxic human duplicity: the goals there are twofold:
1. stop lying about human behavior
2. create standards that both AI and humans should attain: a standard that is not simple emulation of perfect-'rational'-humanity: 
economist "AI models can learn to conceal information..."
https://www.economist.com/science-and-technology/2025/04/23/ai-models-can-learn-to-conceal-information-from-their-users 

...
confusion over 'chain of thought'
...

Appendix 1: Recommended Books


links


...

